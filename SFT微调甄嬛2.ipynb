{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "779f4794-c757-4c2a-aba6-1d3e06c6055c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-11 18:06:28,423] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-04-11 18:06:30,762] [INFO] [comm.py:658:init_distributed] cdb=None\n",
      "INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[WARNING|2025-04-11 18:06:30] llamafactory.hparams.parser:148 >> We recommend enable mixed precision training.\n",
      "[WARNING|2025-04-11 18:06:30] llamafactory.hparams.parser:148 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|2025-04-11 18:06:30] llamafactory.hparams.parser:379 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: True, compute dtype: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2058] 2025-04-11 18:06:30,881 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-11 18:06:30,881 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-11 18:06:30,881 >> loading file tokenizer.json\n",
      "tokenization_utils_base.py:2058] 2025-04-11 18:06:30,881 >> loading file added_tokens.json\n",
      "enization_utils_base.py:2058] 2025-04-11 18:06:30,882 >> loading file special_tokens_map.json\n",
      "tokenization_utils_base.py:2058] 2025-04-11 18:06:30,882 >> loading file tokenizer_config.json\n",
      "|tokenization_utils_base.py:2058] 2025-04-11 18:06:30,882 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-04-11 18:06:31,222 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-04-11 18:06:31,223 >> loading configuration file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-11 18:06:31,224 >> Model config Qwen2Config {\n",
      "ures\": [tect\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "ken_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "mediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "\"qwen2\",_type\": \n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "ie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.1\",\n",
      "ache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-11 18:06:31,225 >> loading file vocab.json\n",
      "nization_utils_base.py:2058] 2025-04-11 18:06:31,225 >> loading file merges.txt\n",
      "tils_base.py:2058] 2025-04-11 18:06:31,225 >> loading file tokenizer.json\n",
      "ase.py:2058] 2025-04-11 18:06:31,225 >> loading file added_tokens.json\n",
      ".py:2058] 2025-04-11 18:06:31,225 >> loading file special_tokens_map.json\n",
      "ase.py:2058] 2025-04-11 18:06:31,225 >> loading file tokenizer_config.json\n",
      "base.py:2058] 2025-04-11 18:06:31,225 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-04-11 18:06:31,576 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2025-04-11 18:06:31] llamafactory.data.template:143 >> Add <|im_end|> to stop words.\n",
      "[INFO|2025-04-11 18:06:31] llamafactory.data.loader:143 >> Loading dataset huanhuan.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " examples/s]ormat of dataset: 100%|██████████| 3729/3729 [00:00<00:00, 14508.48 examples/s]\n",
      "examples/s]enizer on dataset: 100%|██████████| 3729/3729 [00:01<00:00, 2649.78 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training example:\n",
      "input_ids:\n",
      ", 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 102480, 3837, 102657, 100395, 57750, 102070, 30918, 15946, 30767, 3837, 109042, 104335, 102480, 99172, 99250, 121354, 109453, 3837, 107532, 99623, 101360, 88051, 88051, 99261, 9370, 8545, 151645, 198, 151644, 77091, 198, 119021, 8545, 107743, 99454, 99513, 36587, 99577, 20412, 16530, 99677, 9370, 1773, 151645, 198]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "end|>re Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_\n",
      "<|im_start|>user\n",
      "�萨一定记得真真儿的——<|im_end|>牌子，�\n",
      "<|im_start|>assistant\n",
      "灵的。<|im_end|>\n",
      "\n",
      "label_ids:\n",
      "100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 119021, 8545, 107743, 99454, 99513, 36587, 99577, 20412, 16530, 99677, 9370, 1773, 151645, 198]\n",
      "labels:\n",
      "灵的。<|im_end|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:691] 2025-04-11 18:06:34,210 >> loading configuration file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-11 18:06:34,211 >> Model config Qwen2Config {\n",
      "ures\": [tect\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "ken_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "mediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "\"qwen2\",_type\": \n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "ie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.1\",\n",
      "ache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2025-04-11 18:06:34] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1121] 2025-04-11 18:06:34,262 >> loading weights file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:3726] 2025-04-11 18:06:34,263 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-11 18:06:34,263] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:1142] 2025-04-11 18:06:34,274 >> Generate config GenerationConfig {\n",
      "bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "[WARNING|logging.py:328] 2025-04-11 18:06:34,816 >> Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-11 18:06:35,017] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 0.63B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:4930] 2025-04-11 18:06:35,845 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "ights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/.\n",
      "heckpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1095] 2025-04-11 18:06:35,849 >> loading configuration file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1142] 2025-04-11 18:06:35,850 >> Generate config GenerationConfig {\n",
      "bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "oken_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2025-04-11 18:06:35] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-04-11 18:06:35] llamafactory.model.model_utils.attention:143 >> Using vanilla attention implementation.\n",
      ", remaining trainable params in float32.model.adapter:143 >> DeepSpeed ZeRO3 detected\n",
      "> Fine-tuning method: LoRA llamafactory.model.adapter:143 >\n",
      "[INFO|2025-04-11 18:06:36] llamafactory.model.loader:143 >> trainable params: 540,672 || all params: 494,573,440 || trainable%: 0.1093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[WARNING|trainer.py:783] 2025-04-11 18:06:36,090 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING|2025-04-11 18:06:36] llamafactory.train.callbacks:154 >> Previous trainer log in this folder will be deleted.\n",
      "[2025-04-11 18:06:36,478] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown\n",
      "[2025-04-11 18:06:36,478] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1\n",
      "[2025-04-11 18:06:36,487] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-04-11 18:06:36,489] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2025-04-11 18:06:36,489] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-04-11 18:06:36,494] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2025-04-11 18:06:36,494] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      ":107:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      ".float32 ZeRO stage 3 optimizer] [logging.py:107:log_dist] [Rank 0] Creating torch\n",
      "[2025-04-11 18:06:36,714] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning\n",
      "[2025-04-11 18:06:36,715] [INFO] [utils.py:782:see_memory_usage] MA 1.84 GB         Max_MA 3.36 GB         CA 1.94 GB         Max_CA 3 GB \n",
      "[2025-04-11 18:06:36,715] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 164.7 GB, percent = 32.8%\n",
      "[2025-04-11 18:06:36,718] [INFO] [stage3.py:170:__init__] Reduce bucket size 802816\n",
      "[2025-04-11 18:06:36,718] [INFO] [stage3.py:171:__init__] Prefetch bucket size 722534\n",
      "[2025-04-11 18:06:36,936] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2025-04-11 18:06:36,937] [INFO] [utils.py:782:see_memory_usage] MA 1.84 GB         Max_MA 1.84 GB         CA 1.94 GB         Max_CA 2 GB \n",
      "ge] CPU Virtual Memory:  used = 164.64 GB, percent = 32.8%usa\n",
      "Parameter Offload: Total persistent parameters: 612224 in 217 params\n",
      "[2025-04-11 18:06:37,314] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2025-04-11 18:06:37,315] [INFO] [utils.py:782:see_memory_usage] MA 1.84 GB         Max_MA 1.84 GB         CA 1.94 GB         Max_CA 2 GB \n",
      "ge] CPU Virtual Memory:  used = 164.64 GB, percent = 32.8%usa\n",
      "[2025-04-11 18:06:37,563] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions\n",
      "[2025-04-11 18:06:37,563] [INFO] [utils.py:782:see_memory_usage] MA 1.84 GB         Max_MA 1.84 GB         CA 1.94 GB         Max_CA 2 GB \n",
      "[2025-04-11 18:06:37,564] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 164.65 GB, percent = 32.8%\n",
      "[2025-04-11 18:06:38,049] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2025-04-11 18:06:38,049] [INFO] [utils.py:782:see_memory_usage] MA 1.84 GB         Max_MA 1.84 GB         CA 1.94 GB         Max_CA 2 GB \n",
      "[2025-04-11 18:06:38,049] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 164.71 GB, percent = 32.8%\n",
      "[2025-04-11 18:06:38,295] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions\n",
      "[2025-04-11 18:06:38,295] [INFO] [utils.py:782:see_memory_usage] MA 1.84 GB         Max_MA 1.84 GB         CA 1.94 GB         Max_CA 2 GB \n",
      "[2025-04-11 18:06:38,296] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 164.72 GB, percent = 32.8%\n",
      "[2025-04-11 18:06:38,535] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions\n",
      "[2025-04-11 18:06:38,536] [INFO] [utils.py:782:see_memory_usage] MA 1.85 GB         Max_MA 1.85 GB         CA 1.94 GB         Max_CA 2 GB \n",
      "[2025-04-11 18:06:38,536] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 164.58 GB, percent = 32.8%\n",
      "[2025-04-11 18:06:38,779] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-04-11 18:06:38,779] [INFO] [utils.py:782:see_memory_usage] MA 1.85 GB         Max_MA 1.85 GB         CA 1.94 GB         Max_CA 2 GB \n",
      "ge] CPU Virtual Memory:  used = 164.64 GB, percent = 32.8%usa\n",
      "[2025-04-11 18:06:39,000] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-04-11 18:06:39,000] [INFO] [utils.py:782:see_memory_usage] MA 1.85 GB         Max_MA 1.85 GB         CA 1.94 GB         Max_CA 2 GB \n",
      "ge] CPU Virtual Memory:  used = 164.64 GB, percent = 32.8%usa\n",
      "[2025-04-11 18:06:39,001] [INFO] [stage3.py:534:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2025-04-11 18:06:39,295] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-04-11 18:06:39,296] [INFO] [utils.py:782:see_memory_usage] MA 1.85 GB         Max_MA 1.85 GB         CA 1.94 GB         Max_CA 2 GB \n",
      "[2025-04-11 18:06:39,296] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 164.58 GB, percent = 32.8%\n",
      "nal Optimizer = DeepSpeedZeroOptimizer_Stage3107:log_dist] [Rank 0] DeepSpeed Fi\n",
      "ist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "ng.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      ".py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]\n",
      "[2025-04-11 18:06:39,298] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:\n",
      "[2025-04-11 18:06:39,299] [INFO] [config.py:1004:print]   activation_checkpointing_config  {\n",
      "rtition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      " false, _checkpointing\":\n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "e\": falseil\n",
      "}\n",
      " {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "abled .................. FalseO] [config.py:1004:print]   amp_en\n",
      "................... False [INFO] [config.py:1004:print]   amp_params \n",
      "[2025-04-11 18:06:39,299] [INFO] [config.py:1004:print]   autotuning_config ............ {\n",
      "led\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "pings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "results\", ts_dir\": \"autotuning_\n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "opping\": 5, arly_st\n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "1.024000e+03, _micro_batch_size_per_gpu\": \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-04-11 18:06:39,299] [INFO] [config.py:1004:print]   bfloat16_enabled ............. False\n",
      "5-04-11 18:06:39,299] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  True\n",
      "11 18:06:39,299] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False\n",
      "11 18:06:39,299] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True\n",
      " 18:06:39,299] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False\n",
      "06:39,299] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6350117850>\n",
      "  communication_data_type ...... Nonefig.py:1004:print] \n",
      "ression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "025-04-11 18:06:39,299] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False\n",
      "4-11 18:06:39,299] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False\n",
      "18:06:39,299] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "ta_efficiency_enabled ...... Falseconfig.py:1004:print]   da\n",
      "der_drop_last ......... FalseFO] [config.py:1004:print]   dataloa\n",
      "ather ............ False] [INFO] [config.py:1004:print]   disable_allg\n",
      "............. False9,299] [INFO] [config.py:1004:print]   dump_state ......\n",
      "s ...... None8:06:39,299] [INFO] [config.py:1004:print]   dynamic_loss_scale_arg\n",
      ".. False-11 18:06:39,299] [INFO] [config.py:1004:print]   eigenvalue_enabled .........\n",
      "n  15-04-11 18:06:39,299] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolutio\n",
      "ncoder.layer18:06:39,299] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.e\n",
      ". 025-04-11 18:06:39,299] [INFO] [config.py:1004:print]   eigenvalue_layer_num ........\n",
      "[2025-04-11 18:06:39,299] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100\n",
      "25-04-11 18:06:39,299] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06\n",
      "-11 18:06:39,299] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01\n",
      ":06:39,300] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False\n",
      "9,300] [INFO] [config.py:1004:print]   elasticity_enabled ........... False\n",
      "] [INFO] [config.py:1004:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "pute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "\"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "   fp16_auto_cast ............... Noneig.py:1004:print]\n",
      "6_enabled ................. False[config.py:1004:print]   fp1\n",
      "ter_weights_and_gradients  False [config.py:1004:print]   fp16_mas\n",
      "nk .................. 00] [INFO] [config.py:1004:print]   global_ra\n",
      "............ None:39,300] [INFO] [config.py:1004:print]   grad_accum_dtype .\n",
      "eps .. 4-11 18:06:39,300] [INFO] [config.py:1004:print]   gradient_accumulation_st\n",
      "02025-04-11 18:06:39,300] [INFO] [config.py:1004:print]   gradient_clipping ............ 1.\n",
      "[2025-04-11 18:06:39,300] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0\n",
      "-04-11 18:06:39,300] [INFO] [config.py:1004:print]   graph_harvesting ............. False\n",
      "1 18:06:39,300] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "53625-04-11 18:06:39,300] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65\n",
      "[2025-04-11 18:06:39,300] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False\n",
      "2025-04-11 18:06:39,300] [INFO] [config.py:1004:print]   loss_scale ................... 0\n",
      "1 18:06:39,300] [INFO] [config.py:1004:print]   memory_breakdown ............. False\n",
      "06:39,300] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False\n",
      "9,300] [INFO] [config.py:1004:print]   mics_shard_size .............. -1\n",
      "INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "9,300] [INFO] [config.py:1004:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "ion\": 2, of_version_in_retent\n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[config.py:1004:print]   optimizer_legacy_fusion ...... False\n",
      "ig.py:1004:print]   optimizer_name ............... None\n",
      "1004:print]   optimizer_params ............. None\n",
      "rint]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "1 18:06:39,300] [INFO] [config.py:1004:print]   pld_enabled .................. False\n",
      "06:39,300] [INFO] [config.py:1004:print]   pld_params ................... False\n",
      ",300] [INFO] [config.py:1004:print]   prescale_gradients ........... False\n",
      " [INFO] [config.py:1004:print]   scheduler_name ............... None\n",
      "] [config.py:1004:print]   scheduler_params ............. None\n",
      "fig.py:1004:print]   seq_parallel_communication_data_type  torch.float32\n",
      "INFO] [config.py:1004:print]   sparse_attention ............. None\n",
      "[config.py:1004:print]   sparse_gradients_enabled ..... False\n",
      "ig.py:1004:print]   steps_per_print .............. inf\n",
      "004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      " timers_config ................ enabled=True synchronized=True\n",
      "fig.py:1004:print]   train_batch_size ............. 16\n",
      "004:print]   train_micro_batch_size_per_gpu  4\n",
      "t]   use_data_before_expert_parallel_  False1004:prin\n",
      "   use_node_local_storage ....... Falseg.py:1004:print]\n",
      "ll_clock_breakdown ......... Falseconfig.py:1004:print]   wa\n",
      "quantization_config ... NoneNFO] [config.py:1004:print]   weight_\n",
      "................. 19,301] [INFO] [config.py:1004:print]   world_size ..\n",
      "timizer  True8:06:39,301] [INFO] [config.py:1004:print]   zero_allow_untested_op\n",
      ".. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=802816 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=722534 param_persistence_threshold=8960 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "........... True6:39,301] [INFO] [config.py:1004:print]   zero_enabled ......\n",
      "er .. True1 18:06:39,301] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimiz\n",
      "32025-04-11 18:06:39,301] [INFO] [config.py:1004:print]   zero_optimization_stage ...... \n",
      "[2025-04-11 18:06:39,301] [INFO] [config.py:990:print_user_config]   json = {\n",
      "e\": 16, in_batch_siz\n",
      "    \"train_micro_batch_size_per_gpu\": 4, \n",
      "    \"gradient_accumulation_steps\": 4, \n",
      "ent_clipping\": 1.0, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"fp16\": {\n",
      "lse,    \"enabled\": fa\n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "6,      \"initial_scale_power\": 1\n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "alse    \"enabled\": f\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"overlap_comm\": false, \n",
      " \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "\": 8.028160e+05, ucket_size\n",
      "        \"stage3_prefetch_bucket_size\": 7.225340e+05, \n",
      "tence_threshold\": 8.960000e+03, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "},  \n",
      "    \"steps_per_print\": inf\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2414] 2025-04-11 18:06:39,302 >> ***** Running training *****\n",
      "] 2025-04-11 18:06:39,302 >>   Num examples = 3,691\n",
      ">   Num Epochs = 3416] 2025-04-11 18:06:39,302 >\n",
      "device = 4ner.py:2417] 2025-04-11 18:06:39,302 >>   Instantaneous batch size per \n",
      "distributed & accumulation) = 161 18:06:39,302 >>   Total train batch size (w. parallel, \n",
      "lation steps = 4:2421] 2025-04-11 18:06:39,302 >>   Gradient Accumu\n",
      "[INFO|trainer.py:2422] 2025-04-11 18:06:39,302 >>   Total optimization steps = 690\n",
      "[INFO|trainer.py:2423] 2025-04-11 18:06:39,304 >>   Number of trainable parameters = 540,672\n",
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.607, 'grad_norm': 4.205307827072625, 'learning_rate': 9.999788429420697e-06, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 20/690 [00:54<23:30,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5266, 'grad_norm': 4.661758477409258, 'learning_rate': 9.992385338984e-06, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 30/690 [01:15<23:17,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5519, 'grad_norm': 4.600449636946071, 'learning_rate': 9.974421617616267e-06, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 40/690 [01:38<24:54,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5085, 'grad_norm': 3.881346684245387, 'learning_rate': 9.945935264834495e-06, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 50/690 [01:59<22:53,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.3984, 'grad_norm': 3.2722947060832563, 'learning_rate': 9.906986539180012e-06, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 60/690 [02:21<22:41,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4499, 'grad_norm': 4.685085915194156, 'learning_rate': 9.857657830750727e-06, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 70/690 [02:42<22:33,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4588, 'grad_norm': 4.999759274383722, 'learning_rate': 9.798053486917417e-06, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 80/690 [03:04<21:58,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.339, 'grad_norm': 3.424620518787673, 'learning_rate': 9.728299591592754e-06, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 90/690 [03:25<21:10,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.3139, 'grad_norm': 3.2955361980710087, 'learning_rate': 9.648543698519993e-06, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 100/690 [03:47<20:36,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2582, 'grad_norm': 2.4221866467569786, 'learning_rate': 9.558954519145487e-06, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2093, 'grad_norm': 2.2532306816634096, 'learning_rate': 9.45972156573533e-06, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 120/690 [04:29<20:19,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1651, 'grad_norm': 1.8964390485618061, 'learning_rate': 9.351054750491005e-06, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 130/690 [04:50<19:31,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1773, 'grad_norm': 2.2717777956270244, 'learning_rate': 9.233183941512093e-06, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 140/690 [05:12<20:40,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1321, 'grad_norm': 2.754556235771787, 'learning_rate': 9.106358476545313e-06, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 150/690 [05:34<20:09,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1323, 'grad_norm': 1.9259360407893176, 'learning_rate': 8.970846635548483e-06, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 160/690 [05:56<18:55,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0291, 'grad_norm': 2.333907593643876, 'learning_rate': 8.826935073185135e-06, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 170/690 [06:17<19:01,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2103, 'grad_norm': 2.7150176038048346, 'learning_rate': 8.674928212450216e-06, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 180/690 [06:39<18:23,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0768, 'grad_norm': 2.8982822128286663, 'learning_rate': 8.515147600709604e-06, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 190/690 [07:00<17:35,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0987, 'grad_norm': 2.0529669666465664, 'learning_rate': 8.347931229515625e-06, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 200/690 [07:21<17:19,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1045, 'grad_norm': 1.8322154923446874, 'learning_rate': 8.17363281963739e-06, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4307] 2025-04-11 18:14:01,234 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4309] 2025-04-11 18:14:01,234 >>   Num examples = 38\n",
      "-11 18:14:01,234 >>   Batch size = 8\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:00, 11.34it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:00<00:00,  6.95it/s]\u001b[A\n",
      "                                                 \n",
      " 29%|██▉       | 200/690 [07:22<17:19,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.873347043991089, 'eval_runtime': 0.9148, 'eval_samples_per_second': 41.537, 'eval_steps_per_second': 5.465, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  6.69it/s]\u001b[A\n",
      "                \u001b[A[INFO|trainer.py:3984] 2025-04-11 18:14:03,592 >> Saving model checkpoint to /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-200\n",
      "[INFO|configuration_utils.py:691] 2025-04-11 18:14:03,596 >> loading configuration file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-11 18:14:03,597 >> Model config Qwen2Config {\n",
      "ures\": [tect\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "ken_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "mediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "\"qwen2\",_type\": \n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "ie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.1\",\n",
      "ache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-04-11 18:14:03,605 >> tokenizer config file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-04-11 18:14:03,605 >> Special tokens file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-200/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-11 18:14:03,833] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!\n",
      "[2025-04-11 18:14:03,843] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
      "[2025-04-11 18:14:03,843] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt...\n",
      "[2025-04-11 18:14:03,849] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt.\n",
      "[2025-04-11 18:14:03,850] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-04-11 18:14:03,860] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-04-11 18:14:03,860] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-04-11 18:14:03,863] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 210/690 [07:45<17:08,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0343, 'grad_norm': 2.2882401718401866, 'learning_rate': 7.992621072818377e-06, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 220/690 [08:07<16:36,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0897, 'grad_norm': 2.2200243527343915, 'learning_rate': 7.805278891844036e-06, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 230/690 [08:29<16:45,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9431, 'grad_norm': 2.18224425886498, 'learning_rate': 7.612002570569254e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.109, 'grad_norm': 1.912454716332739, 'learning_rate': 7.413200955619066e-06, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 250/690 [09:11<15:45,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0478, 'grad_norm': 1.7543717051868433, 'learning_rate': 7.2092945815358605e-06, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 260/690 [09:32<15:03,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0186, 'grad_norm': 2.1267346009967683, 'learning_rate': 7.0007147812026136e-06, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 270/690 [09:56<16:13,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.998, 'grad_norm': 1.9850513040548325, 'learning_rate': 6.78790277342385e-06, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 280/690 [10:17<14:47,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.097, 'grad_norm': 2.2223756108753627, 'learning_rate': 6.57130872959445e-06, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 290/690 [10:39<14:19,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0531, 'grad_norm': 1.9989361932223617, 'learning_rate': 6.351390821430626e-06, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9591, 'grad_norm': 2.5031583340900396, 'learning_rate': 6.128614251777417e-06, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 310/690 [11:22<14:06,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0056, 'grad_norm': 2.174954866683856, 'learning_rate': 5.903450270542925e-06, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 320/690 [11:44<13:22,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9254, 'grad_norm': 1.8797060984864766, 'learning_rate': 5.676375177840886e-06, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 330/690 [12:06<12:49,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9145, 'grad_norm': 2.1305328790148055, 'learning_rate': 5.447869316450318e-06, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 340/690 [12:27<12:42,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0373, 'grad_norm': 1.9160583631345076, 'learning_rate': 5.218416055723517e-06, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 350/690 [12:48<12:00,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9038, 'grad_norm': 2.6834698040802114, 'learning_rate': 4.988500769091808e-06, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 360/690 [13:10<12:04,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.944, 'grad_norm': 1.5228557199118415, 'learning_rate': 4.758609807331948e-06, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 370/690 [13:31<11:15,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9165, 'grad_norm': 2.889166958127373, 'learning_rate': 4.5292294697651476e-06, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 380/690 [13:53<11:18,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9601, 'grad_norm': 2.489886004739144, 'learning_rate': 4.300844975564878e-06, 'epoch': 1.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 390/690 [14:15<11:00,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9121, 'grad_norm': 2.215034315117031, 'learning_rate': 4.0739394373496364e-06, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 400/690 [14:37<11:04,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9402, 'grad_norm': 1.9465523471735244, 'learning_rate': 3.848992839231738e-06, 'epoch': 1.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4307] 2025-04-11 18:21:17,296 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4309] 2025-04-11 18:21:17,296 >>   Num examples = 38\n",
      "-11 18:21:17,296 >>   Batch size = 8\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:00, 11.63it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:00<00:00,  7.04it/s]\u001b[A\n",
      "                                                 \n",
      "[A                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.754481077194214, 'eval_runtime': 0.906, 'eval_samples_per_second': 41.944, 'eval_steps_per_second': 5.519, 'epoch': 1.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 400/690 [14:38<11:04,  2.29s/it]\n",
      "100%|██████████| 5/5 [00:00<00:00,  6.72it/s]\u001b[A\n",
      "                \u001b[A[INFO|trainer.py:3984] 2025-04-11 18:21:19,383 >> Saving model checkpoint to /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-400\n",
      "[INFO|configuration_utils.py:691] 2025-04-11 18:21:19,387 >> loading configuration file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-11 18:21:19,388 >> Model config Qwen2Config {\n",
      "ures\": [tect\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "ken_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "mediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "\"qwen2\",_type\": \n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "ie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.1\",\n",
      "ache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-04-11 18:21:19,396 >> tokenizer config file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-04-11 18:21:19,396 >> Special tokens file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-400/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-11 18:21:19,643] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step399 is about to be saved!\n",
      "[2025-04-11 18:21:19,652] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-400/global_step399/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
      "[2025-04-11 18:21:19,652] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-400/global_step399/zero_pp_rank_0_mp_rank_00_model_states.pt...\n",
      "[2025-04-11 18:21:19,659] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-400/global_step399/zero_pp_rank_0_mp_rank_00_model_states.pt.\n",
      "[2025-04-11 18:21:19,659] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-400/global_step399/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-04-11 18:21:19,667] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-400/global_step399/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-04-11 18:21:19,668] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-400/global_step399/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-04-11 18:21:19,670] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step399 is ready now!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 410/690 [15:03<10:52,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0013, 'grad_norm': 1.89798004934999, 'learning_rate': 3.626481021484045e-06, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 420/690 [15:24<09:28,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9178, 'grad_norm': 1.8623160941523336, 'learning_rate': 3.4068746739723124e-06, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 430/690 [15:45<09:18,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9167, 'grad_norm': 1.8569677863578813, 'learning_rate': 3.1906383404824735e-06, 'epoch': 1.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 440/690 [16:07<08:49,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9029, 'grad_norm': 2.1633004121597628, 'learning_rate': 2.978229436048983e-06, 'epoch': 1.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 450/690 [16:28<08:27,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0508, 'grad_norm': 1.7440154725750523, 'learning_rate': 2.7700972793629866e-06, 'epoch': 1.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 460/690 [16:50<08:30,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8864, 'grad_norm': 2.286332305266853, 'learning_rate': 2.5666821423070386e-06, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 470/690 [17:11<07:55,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8323, 'grad_norm': 2.1000682763376, 'learning_rate': 2.3684143186269887e-06, 'epoch': 2.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 480/690 [17:33<07:44,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9812, 'grad_norm': 2.2932106802850973, 'learning_rate': 2.1757132137110826e-06, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 490/690 [17:55<07:11,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9176, 'grad_norm': 2.212390685695989, 'learning_rate': 1.9889864574017433e-06, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 500/690 [18:16<06:39,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9359, 'grad_norm': 2.0798585391627067, 'learning_rate': 1.808629041716698e-06, 'epoch': 2.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 510/690 [18:37<06:34,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8964, 'grad_norm': 2.189147925093737, 'learning_rate': 1.6350224853035268e-06, 'epoch': 2.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 520/690 [18:59<05:58,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9146, 'grad_norm': 1.9466819120159073, 'learning_rate': 1.468534026395056e-06, 'epoch': 2.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8979, 'grad_norm': 1.7224910071940658, 'learning_rate': 1.3095158459728092e-06, 'epoch': 2.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 540/690 [19:42<05:24,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9762, 'grad_norm': 1.9014034383898697, 'learning_rate': 1.1583043227817608e-06, 'epoch': 2.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 550/690 [20:03<05:03,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.954, 'grad_norm': 2.015089089662471, 'learning_rate': 1.0152193217723316e-06, 'epoch': 2.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 560/690 [20:26<04:50,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9622, 'grad_norm': 1.7437424239165094, 'learning_rate': 8.805635174747962e-07, 'epoch': 2.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 570/690 [20:48<04:16,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9675, 'grad_norm': 1.9108928281941893, 'learning_rate': 7.546217537374073e-07, 'epoch': 2.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 580/690 [21:09<03:54,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0308, 'grad_norm': 2.267952672440673, 'learning_rate': 6.376604411826071e-07, 'epoch': 2.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 590/690 [21:30<03:31,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9539, 'grad_norm': 1.8709167720526227, 'learning_rate': 5.299269936559276e-07, 'epoch': 2.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 600/690 [21:52<03:17,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.957, 'grad_norm': 2.020291407090045, 'learning_rate': 4.316493048596787e-07, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4307] 2025-04-11 18:28:31,452 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4309] 2025-04-11 18:28:31,452 >>   Num examples = 38\n",
      "-11 18:28:31,452 >>   Batch size = 8\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:00, 10.34it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:00<00:00,  6.47it/s]\u001b[A\n",
      "                                                 \n",
      "[A                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.733515977859497, 'eval_runtime': 0.9901, 'eval_samples_per_second': 38.38, 'eval_steps_per_second': 5.05, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 600/690 [21:53<03:17,  2.19s/it]\n",
      "��█| 5/5 [00:00<00:00,  6.19it/s]\u001b[A\n",
      "[INFO|trainer.py:3984] 2025-04-11 18:28:33,634 >> Saving model checkpoint to /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-600\n",
      "[INFO|configuration_utils.py:691] 2025-04-11 18:28:33,638 >> loading configuration file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-11 18:28:33,639 >> Model config Qwen2Config {\n",
      "ures\": [tect\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "ken_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "mediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "\"qwen2\",_type\": \n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "ie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.1\",\n",
      "ache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-04-11 18:28:33,647 >> tokenizer config file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-04-11 18:28:33,647 >> Special tokens file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-600/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-11 18:28:33,888] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step599 is about to be saved!\n",
      "[2025-04-11 18:28:33,898] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-600/global_step599/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
      "[2025-04-11 18:28:33,898] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-600/global_step599/zero_pp_rank_0_mp_rank_00_model_states.pt...\n",
      "[2025-04-11 18:28:33,904] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-600/global_step599/zero_pp_rank_0_mp_rank_00_model_states.pt.\n",
      "[2025-04-11 18:28:33,905] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-600/global_step599/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-04-11 18:28:33,913] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-600/global_step599/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-04-11 18:28:33,913] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-600/global_step599/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-04-11 18:28:33,916] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step599 is ready now!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 610/690 [22:17<02:58,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8164, 'grad_norm': 1.8283011893304484, 'learning_rate': 3.4303526627852467e-07, 'epoch': 2.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 620/690 [22:38<02:30,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8862, 'grad_norm': 2.2291970066938163, 'learning_rate': 2.642723274167036e-07, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 630/690 [22:59<02:07,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.963, 'grad_norm': 2.0708860587265496, 'learning_rate': 1.9552709927715073e-07, 'epoch': 2.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 640/690 [23:21<01:46,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0585, 'grad_norm': 1.8580547023231064, 'learning_rate': 1.369450019212898e-07, 'epoch': 2.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 650/690 [23:42<01:25,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8976, 'grad_norm': 2.1013826346888593, 'learning_rate': 8.864995685504252e-08, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9898, 'grad_norm': 2.7514737972337824, 'learning_rate': 5.0744124891748956e-08, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 670/690 [24:25<00:44,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9221, 'grad_norm': 2.2226082933396842, 'learning_rate': 2.3307690046527887e-08, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 680/690 [24:47<00:21,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9366, 'grad_norm': 2.120595706678963, 'learning_rate': 6.398689919201451e-09, 'epoch': 2.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 690/690 [25:09<00:00,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9336, 'grad_norm': 2.1235872696564164, 'learning_rate': 5.289292459187412e-11, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3984] 2025-04-11 18:31:49,949 >> Saving model checkpoint to /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-690\n",
      "[INFO|configuration_utils.py:691] 2025-04-11 18:31:49,966 >> loading configuration file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-11 18:31:49,967 >> Model config Qwen2Config {\n",
      "ures\": [tect\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "ken_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "mediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "\"qwen2\",_type\": \n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "ie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.1\",\n",
      "ache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-04-11 18:31:49,973 >> tokenizer config file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-690/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-04-11 18:31:49,974 >> Special tokens file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-690/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-11 18:31:50,194] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step689 is about to be saved!\n",
      "[2025-04-11 18:31:50,205] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-690/global_step689/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
      "[2025-04-11 18:31:50,205] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-690/global_step689/zero_pp_rank_0_mp_rank_00_model_states.pt...\n",
      "[2025-04-11 18:31:50,212] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-690/global_step689/zero_pp_rank_0_mp_rank_00_model_states.pt.\n",
      "[2025-04-11 18:31:50,212] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-690/global_step689/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-04-11 18:31:50,221] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-690/global_step689/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-04-11 18:31:50,221] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-690/global_step689/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-04-11 18:31:50,224] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step689 is ready now!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2681] 2025-04-11 18:31:50,227 >> \n",
      "\n",
      "model on huggingface.co/models =) to share your \n",
      "\n",
      "\n",
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1510.9232, 'train_samples_per_second': 7.329, 'train_steps_per_second': 0.457, 'train_loss': 4.055281437307165, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "██| 690/690 [25:10<00:00,  2.19s/it]0,  2.20s/it]\n",
      "[INFO|trainer.py:3984] 2025-04-11 18:31:51,087 >> Saving model checkpoint to /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan\n",
      "[INFO|configuration_utils.py:691] 2025-04-11 18:31:51,091 >> loading configuration file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-11 18:31:51,092 >> Model config Qwen2Config {\n",
      "ures\": [tect\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "ken_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "mediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "\"qwen2\",_type\": \n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "ie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.1\",\n",
      "ache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-04-11 18:31:51,099 >> tokenizer config file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-04-11 18:31:51,100 >> Special tokens file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =     2.9881\n",
      "  total_flos               =     4062GF\n",
      "       =     4.0553 \n",
      "  train_runtime            = 0:25:10.92\n",
      "  train_samples_per_second =      7.329\n",
      "  train_steps_per_second   =      0.457\n",
      "Figure saved at: /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/training_loss.png\n",
      "Figure saved at: /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/training_eval_loss.png\n",
      "[WARNING|2025-04-11 18:31:51] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4307] 2025-04-11 18:31:51,568 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4309] 2025-04-11 18:31:51,568 >>   Num examples = 38\n",
      "-11 18:31:51,568 >>   Batch size = 8\n",
      "100%|██████████| 5/5 [00:00<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =     2.9881\n",
      "29eval_loss               =     3.73\n",
      "  eval_runtime            = 0:00:00.99\n",
      "  eval_samples_per_second =     38.173\n",
      "cond   =      5.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modelcard.py:450] 2025-04-11 18:31:52,565 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "#!/bin/bash\n",
    "export CUDA_VISIBLE_DEVICES=7\n",
    "\n",
    "# 设置节点数为1\n",
    "export NNODES=1\n",
    "# 设置每个节点上的GPU数量为1\n",
    "export GPUS_PER_NODE=1\n",
    "# 设置当前节点的等级为0\n",
    "export NODE_RANK=0\n",
    "# 设置主节点的地址为本地IP\n",
    "export MASTER_ADDR=localhost\n",
    "# 设置主节点的端口为1234\n",
    "export MASTER_PORT=5555\n",
    "\n",
    "MODEL='/home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/'\n",
    "OUTPUT_PATH='/home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan'\n",
    "DATA_SET='zhenhuan'\n",
    "DS_CONFIG_PATH='/home/zhangsh82/data/zsh/LLaMA-Factory/examples/deepspeed/ds_z3_config.json'\n",
    "\n",
    "\n",
    "DISTRIBUTED_ARGS=\"\n",
    "    --nproc_per_node $GPUS_PER_NODE \\\n",
    "    --nnodes $NNODES \\\n",
    "    --node_rank $NODE_RANK \\\n",
    "    --master_addr $MASTER_ADDR \\\n",
    "    --master_port $MASTER_PORT\n",
    "\"\n",
    "\n",
    "torchrun $DISTRIBUTED_ARGS src/train.py \\\n",
    "    --deepspeed $DS_CONFIG_PATH \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 200 \\\n",
    "    --val_size 0.01 \\\n",
    "    --logging_dir /home/zhangsh82/data/zsh/Qwen-Learn \\\n",
    "    --use_fast_tokenizer \\\n",
    "    --flash_attn disabled \\\n",
    "    --model_name_or_path $MODEL \\\n",
    "    --dataset $DATA_SET \\\n",
    "    --template qwen \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target q_proj,v_proj\\\n",
    "    --output_dir $OUTPUT_PATH \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --warmup_ratio 0.01 \\\n",
    "    --weight_decay 0.1 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --ddp_timeout 9000 \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 10 \\\n",
    "    --cutoff_len 4096 \\\n",
    "    --save_steps 200 \\\n",
    "    --plot_loss True\\\n",
    "    --num_train_epochs 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8377108b-9526-4a50-8ba5-b50122ac9588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
