{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f92f91-ef84-411f-bcd2-59f305c77dd5",
   "metadata": {},
   "source": [
    "# Qwen-VL 端到端微调与优化\n",
    "\n",
    "这份教程的目标，是带大家走完一遍完整的流程，一步步地在自己的数据集上，把 Qwen-VL 模型调好。在这个过程中，我们不只是跑代码，更重要的是搞清楚每一步是在做什么，为什么要这么做。最终的目标是，每个人都能掌握一套可以重复使用、标准化的 VLM 微调工作流。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b34273-6142-4fe1-b4e0-167e1ba17ae2",
   "metadata": {},
   "source": [
    "## 引言：我们的目标\n",
    "\n",
    "\n",
    "1. **环境搭建**：配置一个干净、版本兼容的、可用于 VLM 微调的工作环境。\n",
    "2. **数据处理 (YOLOv8 范式)**：从原始数据源出发，将其转换为 Qwen-VL 微调所需的、包含图像和结构化对话的特定格式，并借鉴 YOLOv8 的思路进行严格的数据校验。\n",
    "3. **模型微调 (SFT & LoRA)**：使用 LLaMA-Factory 或 `trl` 库，在我们的自定义数据上对 Qwen-VL 模型进行高效的 LoRA SFT 微调。\n",
    "4. **结果分析与推理**：学习如何解读训练过程中的损失曲线，并使用微调后的模型对新图片进行预测，验证微调效果。\n",
    "\n",
    "现在，让我们开始吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce0814-2252-4422-91d6-3e8107af38bf",
   "metadata": {},
   "source": [
    "## 第一部分：环境与项目设置\n",
    "\n",
    "一个良好规划的项目结构是成功的一半。我们将首先创建必要的目录，并安装所需的Python库。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f33ab3-daac-423f-8cf8-800e05544261",
   "metadata": {},
   "source": [
    "### 1.1 安装依赖库\n",
    "Qwen-VL 的微调依赖于 transformers, peft, trl 等库。一个稳定且版本兼容的开发环境至关重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c42df1-791d-471b-a92b-9ffa3408f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 核心库安装 (请根据您的 CUDA 版本选择合适的 PyTorch 安装命令)\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Hugging Face 生态系统和微调工具\n",
    "!pip install \"transformers>=4.41.0,<5.0.0\"\n",
    "!pip install -q \"accelerate>=0.30.1\"\n",
    "!pip install -q \"peft>=0.10.0\"\n",
    "!pip install -q \"datasets>=2.19.1\"\n",
    "!pip install -q \"trl\"\n",
    "\n",
    "# Qwen-VL 特定库和性能优化库\n",
    "!pip install -q \"bitsandbytes>=0.43.1\"\n",
    "!pip install -q \"qwen-vl-utils>=0.0.11\"\n",
    "# pip install -q \"flash-attn==2.5.8\" --no-build-isolation # 可选，根据GPU支持情况安装\n",
    "\n",
    "# 可视化和监控工具\n",
    "!pip install -q \"supervision>=0.20.0\"\n",
    "!pip install -q \"tensorboard\"\n",
    "!pip install -q \"wandb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02de70c5-1306-4565-8ce7-0ac1a97fa49b",
   "metadata": {},
   "source": [
    "### 1.2 设置工作目录\n",
    "\n",
    "我们将所有文件（数据、代码、结果）都存放在一个统一的根目录下，这有助于保持项目整洁，并方便路径管理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0fe8c9-5a9f-4124-89f6-42c77f394755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil # 用于文件操作\n",
    "\n",
    "# 尝试处理 Google Colab 环境\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    print(\"--> 正在挂载 Google Drive...\")\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive 挂载成功！\")\n",
    "    # 在 Google Drive 中定义项目根目录以实现持久化存储\n",
    "    BASE_DIR = \"/content/drive/MyDrive/Qwen_VL_Finetune_Project\"\n",
    "except ImportError:\n",
    "    # 在本地环境中，将项目根目录设置在当前工作目录下\n",
    "    BASE_DIR = \"./Qwen_VL_Finetune_Project\"\n",
    "    print(f\"非 Colab 环境，项目根目录设置为: {os.path.abspath(BASE_DIR)}\")\n",
    "\n",
    "# 如果旧目录存在，则清理，确保从干净的状态开始\n",
    "if os.path.exists(BASE_DIR):\n",
    "    print(f\"发现旧的项目目录 '{BASE_DIR}'，正在清理...\")\n",
    "    shutil.rmtree(BASE_DIR)\n",
    "\n",
    "# 创建项目根目录\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "print(f\"项目根目录已创建: {BASE_DIR}\")\n",
    "\n",
    "# 切换当前工作目录至项目根目录，这是非常关键的一步！\n",
    "os.chdir(BASE_DIR)\n",
    "print(f\"当前工作目录已切换至: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c50dcc-cc8e-41a7-ad7f-4cc87f58d172",
   "metadata": {},
   "source": [
    "## 第二部分：数据准备\n",
    "\n",
    "这是整个流程中最核心的一步。将原始数据集转换为 Qwen-VL 可以“理解”的指令对话格式，并进行验证。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92f4091-9b79-402b-801e-6f9e62180720",
   "metadata": {},
   "source": [
    "### 2.1 定义全局配置\n",
    "\n",
    "为了方便管理和修改实验参数，我们将所有重要的配置项都定义在一个地方。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be8531d-7b98-4ec4-accd-1ad34d881aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"管理数据处理的所有配置\"\"\"\n",
    "    # Hugging Face Hub 上的数据集名称\n",
    "    DATASET_NAME: str = \"detection-datasets/coco\"\n",
    "    DATASET_SPLIT: str = \"train\"\n",
    "\n",
    "    # 输出目录配置\n",
    "    OUTPUT_DIR: str = \"qwen_vl_dataset\"\n",
    "    IMAGE_DIR: str = field(init=False)\n",
    "    ANNOTATION_FILE: str = field(init=False)\n",
    "\n",
    "    # 为了快速演示，我们只处理部分样本\n",
    "    NUM_SAMPLES_TO_PROCESS: int = 5000\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.IMAGE_DIR = os.path.join(self.OUTPUT_DIR, \"images\")\n",
    "        self.ANNOTATION_FILE = os.path.join(self.OUTPUT_DIR, \"annotations.jsonl\")\n",
    "\n",
    "# 实例化配置\n",
    "data_cfg = DataConfig()\n",
    "\n",
    "# 创建所有必需的目录\n",
    "os.makedirs(data_cfg.IMAGE_DIR, exist_ok=True)\n",
    "print(f\"数据输出目录结构创建成功！图片将存放于: {data_cfg.IMAGE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b761cd2-0286-4520-a84b-ab0130616e1b",
   "metadata": {},
   "source": [
    "### 2.2 格式转换：从原始标注到指令对话\n",
    "\n",
    "这是最关键的转换逻辑。我们需要将每张图片和它的标注信息转换为 Qwen-VL SFT 所需的对话格式。每个 JSONL 行代表一个样本，包含图片路径和多轮对话。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08638cd5-acc9-4bd4-87a3-7f23ffab366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_sample_to_qwen_vl_format(\n",
    "    sample, image_relative_path, id_to_name, normalize=True\n",
    "):\n",
    "    \"\"\"\n",
    "    将单个样本转换为 Qwen-VL SFT 对话格式。\n",
    "    - normalize=True 时，输出 [0,1] 归一化坐标；否则输出像素坐标。\n",
    "    - 坐标语义为 [x1,y1,x2,y2]，且 x1<x2, y1<y2。\n",
    "    \"\"\"\n",
    "    img_w, img_h = sample[\"image\"].size\n",
    "\n",
    "    # 1) 更强约束的提示词（可按需要保留/删除 <image>）\n",
    "    human_prompt = (\n",
    "        \"<image>\\n\"\n",
    "        \"请找出图中所有目标，并严格以 JSON 数组输出，每个元素为：\"\n",
    "        \"{\\\"bbox_2d\\\":[x1,y1,x2,y2],\\\"label\\\":\\\"类别名\\\"}。\"\n",
    "        f\"{'坐标为归一化到[0,1]' if normalize else '坐标为原图像素'}，\"\n",
    "        \"不要输出任何额外文字。\"\n",
    "    )\n",
    "\n",
    "    # 2) 组装答案\n",
    "    assistant_objects = []\n",
    "    if \"objects\" in sample and sample[\"objects\"].get(\"bbox\"):\n",
    "        for i, (x, y, w, h) in enumerate(sample[\"objects\"][\"bbox\"]):\n",
    "            cat_id = sample[\"objects\"][\"category\"][i]\n",
    "            label_text = id_to_name.get(cat_id, str(cat_id))\n",
    "\n",
    "            x1, y1 = float(x), float(y)\n",
    "            x2, y2 = float(x + w), float(y + h)\n",
    "\n",
    "            # 裁剪到图像边界\n",
    "            x1 = max(0.0, min(x1, img_w))\n",
    "            y1 = max(0.0, min(y1, img_h))\n",
    "            x2 = max(0.0, min(x2, img_w))\n",
    "            y2 = max(0.0, min(y2, img_h))\n",
    "            if x2 <= x1 or y2 <= y1:\n",
    "                continue\n",
    "\n",
    "            if normalize:\n",
    "                bx1, by1 = x1 / img_w, y1 / img_h\n",
    "                bx2, by2 = x2 / img_w, y2 / img_h\n",
    "            else:\n",
    "                bx1, by1, bx2, by2 = int(round(x1)), int(round(y1)), int(round(x2)), int(round(y2))\n",
    "\n",
    "            assistant_objects.append({\"bbox_2d\": [bx1, by1, bx2, by2], \"label\": label_text})\n",
    "\n",
    "    if not assistant_objects:\n",
    "        # 也可选择保留负样本：assistant_objects = []\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"image\": image_relative_path,\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": human_prompt},\n",
    "            {\"from\": \"gpt\", \"value\": json.dumps(assistant_objects, ensure_ascii=False, separators=(\",\", \":\"))},\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "# --- 执行数据处理 ---\n",
    "print(f\"正在从 Hugging Face 加载数据集: {data_cfg.DATASET_NAME}...\")\n",
    "# 使用 streaming=True 避免一次性将整个数据集加载到内存\n",
    "full_dataset = load_dataset(data_cfg.DATASET_NAME, split=data_cfg.DATASET_SPLIT, streaming=True)\n",
    "class_names = load_dataset(data_cfg.DATASET_NAME, split=data_cfg.DATASET_SPLIT).features['objects']['category'].feature.names\n",
    "\n",
    "print(f\"开始处理数据，将生成 {data_cfg.NUM_SAMPLES_TO_PROCESS} 条训练样本...\")\n",
    "with open(data_cfg.ANNOTATION_FILE, 'w', encoding='utf-8') as f:\n",
    "    processed_count = 0\n",
    "    for sample in tqdm(full_dataset, total=data_cfg.NUM_SAMPLES_TO_PROCESS, desc=\"处理样本\"):\n",
    "        if processed_count >= data_cfg.NUM_SAMPLES_TO_PROCESS:\n",
    "            break\n",
    "\n",
    "        image = sample['image']\n",
    "        image_filename = f\"img_{sample['image_id']}.jpg\"\n",
    "        image_save_path = os.path.join(data_cfg.IMAGE_DIR, image_filename)\n",
    "\n",
    "        if not os.path.exists(image_save_path):\n",
    "            if image.mode != 'RGB':\n",
    "                image = image.convert('RGB')\n",
    "            image.save(image_save_path)\n",
    "\n",
    "        conversation = convert_sample_to_qwen_vl_format(sample, image_save_path, class_names)\n",
    "\n",
    "        if conversation:\n",
    "            f.write(json.dumps(conversation, ensure_ascii=False) + '\\n')\n",
    "            processed_count += 1\n",
    "\n",
    "print(f\"\\n数据处理完成！总共生成了 {processed_count} 条训练数据。\")\n",
    "print(f\"标注文件保存在: {data_cfg.ANNOTATION_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8215d9bd-1b7c-4819-bc6c-963e1fa53ef3",
   "metadata": {},
   "source": [
    "### 2.3 数据验证：可视化抽查\n",
    "\n",
    "“Garbage in, garbage out.” 在投入计算资源进行训练之前，验证数据的正确性至关重要。我们将随机抽取几条数据，将其可视化，直观地判断转换是否正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b80d9d5-bd76-45bd-a9e9-9bed0840bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_qwen_vl_sample(jsonl_line):\n",
    "    \"\"\"可视化单个Qwen-VL样本，确认转换是否正确。\"\"\"\n",
    "    try:\n",
    "        data = json.loads(jsonl_line)\n",
    "        image_path = data.get(\"image\")\n",
    "        if not image_path or not os.path.exists(image_path):\n",
    "            print(f\"错误：图像文件 '{image_path}' 不存在。\")\n",
    "            return None\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        \n",
    "        gpt_response_str = data[\"conversations\"][1][\"value\"]\n",
    "        detections = json.loads(gpt_response_str)\n",
    "\n",
    "        colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\"]\n",
    "        \n",
    "        print(f\"在图像 '{os.path.basename(image_path)}' 上找到 {len(detections)} 个物体，正在绘制...\")\n",
    "        for i, det in enumerate(detections):\n",
    "            box = det['bbox_2d']\n",
    "            label = det['label']\n",
    "            color = colors[i % len(colors)]\n",
    "            \n",
    "            draw.rectangle(box, outline=color, width=3)\n",
    "            \n",
    "            try:\n",
    "                font = ImageFont.truetype(\"arial.ttf\", 15)\n",
    "            except IOError:\n",
    "                font = ImageFont.load_default()\n",
    "            \n",
    "            text_bbox = draw.textbbox((box[0], box[1] - 15), label, font=font)\n",
    "            draw.rectangle(text_bbox, fill=color)\n",
    "            draw.text((box[0], box[1] - 15), label, fill=\"black\", font=font)\n",
    "            \n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"可视化时发生错误: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 执行可视化验证 ---\n",
    "print(\"\\n正在随机抽查1条数据进行可视化验证...\")\n",
    "with open(data_cfg.ANNOTATION_FILE, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    if not lines:\n",
    "        print(\"标注文件为空，无法验证。\")\n",
    "    else:\n",
    "        random_line = random.choice(lines)\n",
    "        visualized_image = visualize_qwen_vl_sample(random_line)\n",
    "        if visualized_image:\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(visualized_image)\n",
    "            plt.axis('off')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092390de-3e5f-4bd6-aa55-040accc1faa9",
   "metadata": {},
   "source": [
    "## 第三部分：监督微调 (SFT)\n",
    "\n",
    "数据准备就绪，现在可以开始训练了！我们将使用 Hugging Face trl 库中的 SFTTrainer 进行第一阶段的微调。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38c457-4c4d-40c6-a772-db4186d7ea84",
   "metadata": {},
   "source": [
    "### 3.1 方法一：使用 Hugging Face TRL 进行微调\n",
    "trl 库是 Hugging Face 官方推荐的微调工具库，它与 transformers 和 peft 无缝集成，通过 SFTTrainer 可以用非常简洁的代码实现微调。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1143fdd5-8d70-4e95-80ba-d5b66272c7cd",
   "metadata": {},
   "source": [
    "#### 3.1.1 定义 SFT 训练配置\n",
    "\n",
    "我们将所有的训练超参数集中在一个配置类中，方便管理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85194491-b03b-4f4c-b136-8fe7a761243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class SFTConfig:\n",
    "    # 模型与数据路径\n",
    "    model_id: str = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "    data_path: str = data_cfg.ANNOTATION_FILE\n",
    "    output_dir: str = \"./qwen2.5-vl-sft-lora\"\n",
    "\n",
    "    # 训练循环控制\n",
    "    num_train_epochs: int = 1\n",
    "    per_device_train_batch_size: int = 2 # 根据显存调整\n",
    "    gradient_accumulation_steps: int = 8 # 有效批大小 = 2 * 8 = 16\n",
    "    gradient_checkpointing: bool = True\n",
    "\n",
    "    # 优化器与学习率\n",
    "    learning_rate: float = 2e-5\n",
    "    weight_decay: float = 0.\n",
    "    warmup_ratio: float = 0.03\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    optim: str = \"paged_adamw_8bit\"\n",
    "    max_grad_norm: float = 1.0\n",
    "\n",
    "    # LoRA 配置\n",
    "    lora_rank: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "\n",
    "    # Q-LoRA 和精度配置\n",
    "    bits: int = 4\n",
    "    bf16: bool = True # 如果GPU支持，bf16优于fp16\n",
    "\n",
    "    # 其他\n",
    "    logging_steps: int = 10\n",
    "    save_steps: int = 100\n",
    "    max_seq_length: int = 1024\n",
    "    report_to: str = \"tensorboard\"\n",
    "\n",
    "# 实例化训练配置\n",
    "sft_train_cfg = SFTConfig()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9811b77e-e334-4cbc-aa2b-af28e6c77392",
   "metadata": {},
   "source": [
    "#### 3.1.2 执行 SFT 训练\n",
    "\n",
    "这部分整合了模型的加载、LoRA 的配置以及 SFTTrainer 的初始化和启动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7fb575-c0b2-47fe-a5f9-8d58e7dc46a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig as TrlSFTConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. 配置4位量化 (Q-LoRA)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# 2. 加载模型和处理器\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    sft_train_cfg.model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(sft_train_cfg.model_id, trust_remote_code=True)\n",
    "if processor.tokenizer.pad_token is None:\n",
    "    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "\n",
    "# 3. 加载数据集\n",
    "raw_datasets = load_dataset(\"json\", data_files=sft_train_cfg.data_path, split=\"train\")\n",
    "\n",
    "# 4. 配置并应用 LoRA\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    r=sft_train_cfg.lora_rank,\n",
    "    lora_alpha=sft_train_cfg.lora_alpha,\n",
    "    lora_dropout=sft_train_cfg.lora_dropout,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 5. 配置 SFTTrainer\n",
    "training_args = TrlSFTConfig(\n",
    "    output_dir=sft_train_cfg.output_dir,\n",
    "    num_train_epochs=sft_train_cfg.num_train_epochs,\n",
    "    per_device_train_batch_size=sft_train_cfg.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=sft_train_cfg.gradient_accumulation_steps,\n",
    "    gradient_checkpointing=sft_train_cfg.gradient_checkpointing,\n",
    "    optim=sft_train_cfg.optim,\n",
    "    logging_steps=sft_train_cfg.logging_steps,\n",
    "    learning_rate=sft_train_cfg.learning_rate,\n",
    "    bf16=sft_train_cfg.bf16,\n",
    "    max_grad_norm=sft_train_cfg.max_grad_norm,\n",
    "    warmup_ratio=sft_train_cfg.warmup_ratio,\n",
    "    lr_scheduler_type=sft_train_cfg.lr_scheduler_type,\n",
    "    save_steps=sft_train_cfg.save_steps,\n",
    "    max_seq_length=sft_train_cfg.max_seq_length,\n",
    "    dataset_text_field=\"conversations\",\n",
    "    dataset_kwargs={\"image_column\": \"image\"},\n",
    "    report_to=sft_train_cfg.report_to\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=raw_datasets,\n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "# 6. 启动训练\n",
    "print(\"--- 开始SFT微调 ---\")\n",
    "trainer.train()\n",
    "print(\"--- SFT微调完成！ ---\")\n",
    "\n",
    "# 7. 保存适配器\n",
    "trainer.save_model(sft_train_cfg.output_dir)\n",
    "print(f\"SFT LoRA 适配器已保存至: {sft_train_cfg.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d440b40d-2f24-4f22-a31c-4d322a373466",
   "metadata": {},
   "source": [
    "### 3.2 方法二：使用 LLaMA-Factory 进行微调\n",
    "LLaMA-Factory 是一个集成了多种高效微调策略的开源框架。它通过命令行工具 llamafactory-cli 提供了强大的功能，非常适合进行系统化的实验。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044df86e-8ab8-46dc-8559-3b8ff8b84b96",
   "metadata": {},
   "source": [
    "#### 3.2.1 安装 LLaMA-Factory\n",
    "首先，我们需要从 GitHub 克隆仓库并进行安装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c198b67-639d-45ba-aec8-60f39fc807a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "!pip install -e LLaMA-Factory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb474c3-dfd7-442a-b92f-3adb1187b3ee",
   "metadata": {},
   "source": [
    "#### 3.2.2 执行 SFT 训练\n",
    "准备好数据配置文件后，我们可以用一个 Shell 脚本来启动训练。这种方式的好处是所有的超参数都一目了然，便于修改和记录。\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# --- LLaMA-Factory SFT 训练脚本 ---\n",
    "\n",
    "# 模型和数据路径\n",
    "MODEL_ID=\"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "# 这是我们在 2.4 节中 dataset_info.json 里定义的名字\n",
    "DATASET_NAME=\"qwen_vl_detection\"\n",
    "# 包含 dataset_info.json 和 annotations.jsonl 的目录\n",
    "DATA_DIR=\"qwen_vl_dataset\"\n",
    "# 训练输出目录\n",
    "OUTPUT_DIR=\"./qwen2.5-vl-sft-lora-llamafactory\"\n",
    "\n",
    "# 启动训练\n",
    "llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path $MODEL_ID \\\n",
    "    --dataset $DATASET_NAME \\\n",
    "    --dataset_dir $DATA_DIR \\\n",
    "    --template qwen \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target q_proj v_proj \\\n",
    "    --output_dir $OUTPUT_DIR \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --plot_loss \\\n",
    "    --bf16 \\\n",
    "    --max_seq_length 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3086d793-8892-4a51-8104-838e79ceffc7",
   "metadata": {},
   "source": [
    "## 第四部分：推理与部署\n",
    "\n",
    "经过 SFT 微调后，我们的模型已经准备好了。最后一步是合并 LoRA 权重并进行最终的推理验证。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1d2911-e068-44bd-9b7d-73326fa55706",
   "metadata": {},
   "source": [
    "### 4.1 合并 SFT 权重\n",
    "\n",
    "这个步骤将 LoRA 适配器的权重融入基础模型，生成一个独立的、可直接部署的完整模型。请注意：无论你使用 TRL 还是 LLaMA-Factory，合并权重的代码都是一样的，只需要修改 `sft_train_cfg.output_dir` 为你实际的 LoRA 权重输出目录即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f0188-d97b-4097-ba1a-eb1a738bfda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "\n",
    "# --- 配置 ---\n",
    "# 根据你使用的训练方法，选择对应的LoRA输出目录\n",
    "# LORA_ADAPTER_PATH = sft_train_cfg.output_dir # 使用 TRL 训练的\n",
    "LORA_ADAPTER_PATH = \"./qwen2.5-vl-sft-lora-llamafactory\" # 使用 LLaMA-Factory 训练的\n",
    "\n",
    "MERGED_MODEL_PATH = \"./qwen2.5-vl-sft-merged\"\n",
    "\n",
    "# --- 加载基础模型 ---\n",
    "base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    sft_train_cfg.model_id, # model_id 是一样的\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# --- 加载SFT适配器并合并 ---\n",
    "final_model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)\n",
    "merged_model = final_model.merge_and_unload()\n",
    "\n",
    "# --- 保存最终模型 ---\n",
    "merged_model.save_pretrained(MERGED_MODEL_PATH)\n",
    "processor.save_pretrained(MERGED_MODEL_PATH)\n",
    "print(f\"最终合并后的模型已保存至: {MERGED_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ae95b5-2f4c-43c2-ab5c-b72b39286c7a",
   "metadata": {},
   "source": [
    "### 4.2 使用最终模型进行推理\n",
    "现在，我们可以用最终合并好的模型来进行推理，看看效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7e5e62-cc6c-4d30-b08e-e32167847093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 加载最终合并后的模型 ---\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MERGED_MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(MERGED_MODEL_PATH, trust_remote_code=True)\n",
    "\n",
    "# --- 准备推理输入 ---\n",
    "with open(data_cfg.ANNOTATION_FILE, 'r', encoding='utf-8') as f:\n",
    "    test_line = f.readlines()[0]\n",
    "    test_data = json.loads(test_line)\n",
    "    \n",
    "image_path = test_data['image']\n",
    "image = Image.open(image_path)\n",
    "\n",
    "prompt = \"<image>\\n这张图片里有什么？请给出它们的边界框。\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = processor(text=text_input, images=image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# --- 执行推理 ---\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "response_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "print(\"--- 最终模型输出 ---\")\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11664bc5-3a94-4bed-92af-1ad935918acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
