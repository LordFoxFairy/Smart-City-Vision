{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98510b5a-7698-406d-9d08-89671e86beca",
   "metadata": {},
   "source": [
    "# SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a12ba83-a2c9-4bd9-91b8-5d9013834463",
   "metadata": {},
   "source": [
    "# Lora-sft微调千问0.5B，FP16半精度，显存使用7GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49675f21-e644-40d0-8500-701eb990dd5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0411 17:18:09.312000 47181 site-packages/torch/distributed/run.py:793] \n",
      " site-packages/torch/distributed/run.py:793] *****************************************\n",
      "9.312000 47181 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "e-packages/torch/distributed/run.py:793] *****************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-11 17:18:13,812] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-04-11 17:18:13,930] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-04-11 17:18:16,327] [INFO] [comm.py:658:init_distributed] cdb=None\n",
      "[2025-04-11 17:18:16,517] [INFO] [comm.py:658:init_distributed] cdb=None\n",
      "[2025-04-11 17:18:16,517] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[WARNING|2025-04-11 17:18:16] llamafactory.hparams.parser:148 >> We recommend enable mixed precision training.\n",
      "[WARNING|2025-04-11 17:18:16] llamafactory.hparams.parser:148 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|2025-04-11 17:18:16] llamafactory.hparams.parser:379 >> Process rank: 0, world size: 2, device: cuda:0, distributed training: True, compute dtype: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2058] 2025-04-11 17:18:16,740 >> loading file vocab.json\n",
      "nization_utils_base.py:2058] 2025-04-11 17:18:16,740 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-11 17:18:16,740 >> loading file tokenizer.json\n",
      "tokenization_utils_base.py:2058] 2025-04-11 17:18:16,740 >> loading file added_tokens.json\n",
      "enization_utils_base.py:2058] 2025-04-11 17:18:16,740 >> loading file special_tokens_map.json\n",
      "tokenization_utils_base.py:2058] 2025-04-11 17:18:16,740 >> loading file tokenizer_config.json\n",
      "|tokenization_utils_base.py:2058] 2025-04-11 17:18:16,740 >> loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2025-04-11 17:18:16] llamafactory.hparams.parser:379 >> Process rank: 1, world size: 2, device: cuda:1, distributed training: True, compute dtype: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2323] 2025-04-11 17:18:17,115 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-04-11 17:18:17,116 >> loading configuration file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-11 17:18:17,118 >> Model config Qwen2Config {\n",
      "ures\": [tect\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "ken_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "mediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "\"qwen2\",_type\": \n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "ie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.1\",\n",
      "ache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-11 17:18:17,118 >> loading file vocab.json\n",
      "nization_utils_base.py:2058] 2025-04-11 17:18:17,119 >> loading file merges.txt\n",
      "tils_base.py:2058] 2025-04-11 17:18:17,119 >> loading file tokenizer.json\n",
      "ase.py:2058] 2025-04-11 17:18:17,119 >> loading file added_tokens.json\n",
      ".py:2058] 2025-04-11 17:18:17,119 >> loading file special_tokens_map.json\n",
      "ase.py:2058] 2025-04-11 17:18:17,119 >> loading file tokenizer_config.json\n",
      "base.py:2058] 2025-04-11 17:18:17,119 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-04-11 17:18:17,556 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[rank1]:[W411 17:18:17.349048436 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2025-04-11 17:18:17] llamafactory.data.template:143 >> Add <|im_end|> to stop words.\n",
      "[INFO|2025-04-11 17:18:17] llamafactory.data.loader:143 >> Loading dataset huanhuan.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " examples/s]ormat of dataset: 100%|██████████| 3729/3729 [00:00<00:00, 16168.14 examples/\n",
      "[rank0]:[W411 17:18:18.514925625 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "examples/s]enizer on dataset: 100%|██████████| 3729/3729 [00:01<00:00, 2562.89 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training example:\n",
      "input_ids:\n",
      ", 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 102480, 3837, 102657, 100395, 57750, 102070, 30918, 15946, 30767, 3837, 109042, 104335, 102480, 99172, 99250, 121354, 109453, 3837, 107532, 99623, 101360, 88051, 88051, 99261, 9370, 8545, 151645, 198, 151644, 77091, 198, 119021, 8545, 107743, 99454, 99513, 36587, 99577, 20412, 16530, 99677, 9370, 1773, 151645, 198]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "end|>re Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_\n",
      "<|im_start|>user\n",
      "�萨一定记得真真儿的——<|im_end|>牌子，�\n",
      "<|im_start|>assistant\n",
      "灵的。<|im_end|>\n",
      "\n",
      "label_ids:\n",
      "100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 119021, 8545, 107743, 99454, 99513, 36587, 99577, 20412, 16530, 99677, 9370, 1773, 151645, 198]\n",
      "labels:\n",
      "灵的。<|im_end|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:691] 2025-04-11 17:18:20,771 >> loading configuration file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-11 17:18:20,773 >> Model config Qwen2Config {\n",
      "ures\": [tect\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "ken_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "mediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "\"qwen2\",_type\": \n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "ie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.1\",\n",
      "ache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2025-04-11 17:18:20] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1121] 2025-04-11 17:18:20,828 >> loading weights file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:3726] 2025-04-11 17:18:20,829 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-11 17:18:20,830] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:1142] 2025-04-11 17:18:20,842 >> Generate config GenerationConfig {\n",
      "bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-11 17:18:20,922] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|logging.py:328] 2025-04-11 17:18:20,924 >> Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-11 17:18:21,155] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 0.63B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:4930] 2025-04-11 17:18:22,633 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "ights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/.\n",
      "heckpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1095] 2025-04-11 17:18:22,636 >> loading configuration file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1142] 2025-04-11 17:18:22,636 >> Generate config GenerationConfig {\n",
      "bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "oken_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2025-04-11 17:18:22] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "attention implementation.] llamafactory.model.model_utils.attention:143 >> Using vanilla \n",
      "O3 detected, remaining trainable params in float32.er:143 >> DeepSpeed ZeR\n",
      "apter:143 >> Fine-tuning method: LoRAry.model.ad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2025-04-11 17:18:22] llamafactory.model.loader:143 >> trainable params: 540,672 || all params: 494,573,440 || trainable%: 0.1093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[WARNING|trainer.py:783] 2025-04-11 17:18:22,882 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-11 17:18:23,213] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown\n",
      "[2025-04-11 17:18:23,214] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 2\n",
      "[2025-04-11 17:18:23,223] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-04-11 17:18:23,224] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2025-04-11 17:18:23,224] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-04-11 17:18:23,230] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2025-04-11 17:18:23,230] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      ":107:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      ".float32 ZeRO stage 3 optimizer] [logging.py:107:log_dist] [Rank 0] Creating torch\n",
      "[2025-04-11 17:18:23,480] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning\n",
      "[2025-04-11 17:18:23,480] [INFO] [utils.py:782:see_memory_usage] MA 0.93 GB         Max_MA 2.2 GB         CA 0.97 GB         Max_CA 2 GB \n",
      "[2025-04-11 17:18:23,481] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 172.25 GB, percent = 34.3%\n",
      "[2025-04-11 17:18:23,484] [INFO] [stage3.py:170:__init__] Reduce bucket size 802816\n",
      "[2025-04-11 17:18:23,484] [INFO] [stage3.py:171:__init__] Prefetch bucket size 722534\n",
      "[2025-04-11 17:18:23,733] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2025-04-11 17:18:23,734] [INFO] [utils.py:782:see_memory_usage] MA 0.93 GB         Max_MA 0.93 GB         CA 0.97 GB         Max_CA 1 GB \n",
      "ge] CPU Virtual Memory:  used = 172.32 GB, percent = 34.3%usa\n",
      "Parameter Offload: Total persistent parameters: 612224 in 217 params\n",
      "[2025-04-11 17:18:24,143] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2025-04-11 17:18:24,144] [INFO] [utils.py:782:see_memory_usage] MA 0.93 GB         Max_MA 0.93 GB         CA 0.97 GB         Max_CA 1 GB \n",
      "ge] CPU Virtual Memory:  used = 172.2 GB, percent = 34.3%_usa\n",
      "[2025-04-11 17:18:24,435] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions\n",
      "[2025-04-11 17:18:24,435] [INFO] [utils.py:782:see_memory_usage] MA 0.93 GB         Max_MA 0.93 GB         CA 0.97 GB         Max_CA 1 GB \n",
      "[2025-04-11 17:18:24,436] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 172.29 GB, percent = 34.3%\n",
      "[2025-04-11 17:18:25,060] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2025-04-11 17:18:25,061] [INFO] [utils.py:782:see_memory_usage] MA 0.93 GB         Max_MA 0.93 GB         CA 0.97 GB         Max_CA 1 GB \n",
      "[2025-04-11 17:18:25,061] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 172.32 GB, percent = 34.3%\n",
      "[2025-04-11 17:18:25,318] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions\n",
      "[2025-04-11 17:18:25,319] [INFO] [utils.py:782:see_memory_usage] MA 0.93 GB         Max_MA 0.93 GB         CA 0.97 GB         Max_CA 1 GB \n",
      "[2025-04-11 17:18:25,319] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 172.21 GB, percent = 34.3%\n",
      "[2025-04-11 17:18:25,573] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions\n",
      "[2025-04-11 17:18:25,574] [INFO] [utils.py:782:see_memory_usage] MA 0.93 GB         Max_MA 0.93 GB         CA 0.97 GB         Max_CA 1 GB \n",
      "[2025-04-11 17:18:25,574] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 172.19 GB, percent = 34.3%\n",
      "[2025-04-11 17:18:25,829] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-04-11 17:18:25,832] [INFO] [utils.py:782:see_memory_usage] MA 0.93 GB         Max_MA 0.93 GB         CA 0.97 GB         Max_CA 1 GB \n",
      "[2025-04-11 17:18:25,832] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 172.22 GB, percent = 34.3%\n",
      "[2025-04-11 17:18:26,090] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-04-11 17:18:26,091] [INFO] [utils.py:782:see_memory_usage] MA 0.93 GB         Max_MA 0.93 GB         CA 0.97 GB         Max_CA 1 GB \n",
      "ge] CPU Virtual Memory:  used = 172.17 GB, percent = 34.3%usa\n",
      "[2025-04-11 17:18:26,092] [INFO] [stage3.py:534:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2025-04-11 17:18:26,406] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-04-11 17:18:26,407] [INFO] [utils.py:782:see_memory_usage] MA 0.93 GB         Max_MA 0.93 GB         CA 0.97 GB         Max_CA 1 GB \n",
      "[2025-04-11 17:18:26,407] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 172.29 GB, percent = 34.3%\n",
      "nal Optimizer = DeepSpeedZeroOptimizer_Stage3107:log_dist] [Rank 0] DeepSpeed Fi\n",
      "ist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "ng.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      ".py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]\n",
      "[2025-04-11 17:18:26,409] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:\n",
      "7:18:26,410] [INFO] [config.py:1004:print]   activation_checkpointing_config  {\n",
      "ations\": false, ctiv\n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "\"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-04-11 17:18:26,410] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "............ False26,410] [INFO] [config.py:1004:print]   amp_enabled ......\n",
      "....... False7:18:26,410] [INFO] [config.py:1004:print]   amp_params ............\n",
      ".. {5-04-11 17:18:26,410] [INFO] [config.py:1004:print]   autotuning_config ..........\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null,\n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      ": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "e,  \"fast\": tru\n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      " 1, \"mp_size\":\n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "ize_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "ch_sizes\": 3ing_micro_bat\n",
      "}\n",
      "... False11 17:18:26,410] [INFO] [config.py:1004:print]   bfloat16_enabled ..........\n",
      "True5-04-11 17:18:26,410] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  \n",
      "alse5-04-11 17:18:26,410] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  F\n",
      "ue025-04-11 17:18:26,410] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  Tr\n",
      "[2025-04-11 17:18:26,410] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False\n",
      "2025-04-11 17:18:26,410] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fde8006f310>\n",
      "y:1004:print]   communication_data_type ...... None\n",
      ":print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      " False04-11 17:18:26,410] [INFO] [config.py:1004:print]   curriculum_enabled_legacy ....\n",
      "e2025-04-11 17:18:26,410] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... Fals\n",
      "': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "04:print]   data_efficiency_enabled ...... False\n",
      "int]   dataloader_drop_last ......... False:1004:pr\n",
      "  disable_allgather ............ Falseig.py:1004:print] \n",
      "p_state ................... False[config.py:1004:print]   dum\n",
      "loss_scale_args ...... NoneINFO] [config.py:1004:print]   dynamic_\n",
      "bled ........... False10] [INFO] [config.py:1004:print]   eigenvalue_ena\n",
      "dary_resolution  126,410] [INFO] [config.py:1004:print]   eigenvalue_gas_boun\n",
      "....... bert.encoder.layer[INFO] [config.py:1004:print]   eigenvalue_layer_name .\n",
      "r_num ......... 0:26,410] [INFO] [config.py:1004:print]   eigenvalue_laye\n",
      "...... 1001 17:18:26,410] [INFO] [config.py:1004:print]   eigenvalue_max_iter ....\n",
      "1e-06-04-11 17:18:26,410] [INFO] [config.py:1004:print]   eigenvalue_stability ......... \n",
      "[2025-04-11 17:18:26,410] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-04-11 17:18:26,410] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False\n",
      "-04-11 17:18:26,410] [INFO] [config.py:1004:print]   elasticity_enabled ........... False\n",
      "1 17:18:26,410] [INFO] [config.py:1004:print]   flops_profiler_config ........ {\n",
      "e,  \"enabled\": fals\n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "les\": 1, modu\n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "py:1004:print]   fp16_auto_cast ............... None\n",
      "4:print]   fp16_enabled ................. False\n",
      "nt]   fp16_master_weights_and_gradients  False04:pri\n",
      "t]   global_rank .................. 0fig.py:1004:prin\n",
      "_accum_dtype ............. None] [config.py:1004:print]   grad\n",
      "ccumulation_steps .. 411] [INFO] [config.py:1004:print]   gradient_a\n",
      "........... 1.018:26,411] [INFO] [config.py:1004:print]   gradient_clipping .\n",
      ".... 1.0-11 17:18:26,411] [INFO] [config.py:1004:print]   gradient_predivide_factor \n",
      "lse25-04-11 17:18:26,411] [INFO] [config.py:1004:print]   graph_harvesting ............. Fa\n",
      "=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-04-11 17:18:26,411] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 65536\n",
      "-04-11 17:18:26,411] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False\n",
      "[2025-04-11 17:18:26,411] [INFO] [config.py:1004:print]   loss_scale ................... 0\n",
      "11 17:18:26,411] [INFO] [config.py:1004:print]   memory_breakdown ............. False\n",
      ":18:26,411] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False\n",
      "26,411] [INFO] [config.py:1004:print]   mics_shard_size .............. -1\n",
      "[INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "26,411] [INFO] [config.py:1004:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "\"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "tion\": 2, f_version_in_reten\n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      " [config.py:1004:print]   optimizer_legacy_fusion ...... False\n",
      "fig.py:1004:print]   optimizer_name ............... None\n",
      ":1004:print]   optimizer_params ............. None\n",
      "print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "11 17:18:26,411] [INFO] [config.py:1004:print]   pld_enabled .................. False\n",
      ":18:26,411] [INFO] [config.py:1004:print]   pld_params ................... False\n",
      "6,411] [INFO] [config.py:1004:print]   prescale_gradients ........... False\n",
      "] [INFO] [config.py:1004:print]   scheduler_name ............... None\n",
      "O] [config.py:1004:print]   scheduler_params ............. None\n",
      "nfig.py:1004:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[INFO] [config.py:1004:print]   sparse_attention ............. None\n",
      " [config.py:1004:print]   sparse_gradients_enabled ..... False\n",
      "fig.py:1004:print]   steps_per_print .............. inf\n",
      "1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      "  timers_config ................ enabled=True synchronized=True\n",
      "nfig.py:1004:print]   train_batch_size ............. 32\n",
      "1004:print]   train_micro_batch_size_per_gpu  4\n",
      "nt]   use_data_before_expert_parallel_  False004:pri\n",
      "]   use_node_local_storage ....... False.py:1004:print\n",
      "all_clock_breakdown ......... Falseonfig.py:1004:print]   w\n",
      "_quantization_config ... NoneFO] [config.py:1004:print]   weight\n",
      ".................. 2,411] [INFO] [config.py:1004:print]   world_size .\n",
      "ptimizer  True:18:26,411] [INFO] [config.py:1004:print]   zero_allow_untested_o\n",
      "... stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=802816 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=722534 param_persistence_threshold=8960 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "............ True:26,412] [INFO] [config.py:1004:print]   zero_enabled .....\n",
      "zer .. True 17:18:26,412] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimi\n",
      " 3025-04-11 17:18:26,412] [INFO] [config.py:1004:print]   zero_optimization_stage ......\n",
      "[2025-04-11 17:18:26,412] [INFO] [config.py:990:print_user_config]   json = {\n",
      "ze\": 32, n_batch_si\n",
      "    \"train_micro_batch_size_per_gpu\": 4, \n",
      "    \"gradient_accumulation_steps\": 4, \n",
      "ient_clipping\": 1.0, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"fp16\": {\n",
      "alse,   \"enabled\": f\n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "16,     \"initial_scale_power\": \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "false   \"enabled\": \n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"overlap_comm\": false, \n",
      "  \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "e\": 8.028160e+05, cket_siz\n",
      "        \"stage3_prefetch_bucket_size\": 7.225340e+05, \n",
      "stence_threshold\": 8.960000e+03, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "e3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      " }, \n",
      "    \"steps_per_print\": inf\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2414] 2025-04-11 17:18:26,413 >> ***** Running training *****\n",
      "] 2025-04-11 17:18:26,413 >>   Num examples = 3,729\n",
      ">   Num Epochs = 1016] 2025-04-11 17:18:26,413 >\n",
      " device = 4er.py:2417] 2025-04-11 17:18:26,413 >>   Instantaneous batch size per\n",
      " distributed & accumulation) = 32 17:18:26,413 >>   Total train batch size (w. parallel,\n",
      "ulation steps = 42421] 2025-04-11 17:18:26,413 >>   Gradient Accum\n",
      "60NFO|trainer.py:2422] 2025-04-11 17:18:26,413 >>   Total optimization steps = 1,1\n",
      "[INFO|trainer.py:2423] 2025-04-11 17:18:26,415 >>   Number of trainable parameters = 540,672\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6024, 'grad_norm': 4.535499636624993, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4264, 'grad_norm': 3.5743929939698478, 'learning_rate': 4.9995413210794864e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 30/1160 [01:24<45:40,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2755, 'grad_norm': 1.6257038527464418, 'learning_rate': 4.997295135614539e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 40/1160 [01:48<45:03,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1141, 'grad_norm': 1.8651777219408507, 'learning_rate': 4.993178876386278e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 50/1160 [02:12<44:24,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1116, 'grad_norm': 1.7410576680540801, 'learning_rate': 4.987195625813066e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 60/1160 [02:36<44:45,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0314, 'grad_norm': 1.4041306404166194, 'learning_rate': 4.9793498643905236e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 70/1160 [03:01<44:19,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9164, 'grad_norm': 1.4908513851907301, 'learning_rate': 4.9696474673363536e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9587, 'grad_norm': 1.3155281006854105, 'learning_rate': 4.9580957001907445e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.903, 'grad_norm': 1.5901412210006791, 'learning_rate': 4.944703213375648e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 100/1160 [04:14<43:26,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8931, 'grad_norm': 1.6504634940090541, 'learning_rate': 4.929480035716997e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 110/1160 [04:38<42:40,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8432, 'grad_norm': 1.656492930788128, 'learning_rate': 4.912437566934723e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 120/1160 [05:03<42:38,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.268, 'grad_norm': 1.6434239833318922, 'learning_rate': 4.893588569106195e-05, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 130/1160 [05:27<41:44,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8061, 'grad_norm': 1.5687891147002724, 'learning_rate': 4.872947157109467e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.847, 'grad_norm': 1.75477123719886, 'learning_rate': 4.850528788053495e-05, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 150/1160 [06:16<40:51,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.803, 'grad_norm': 1.7230835279298375, 'learning_rate': 4.8263502497032484e-05, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 160/1160 [06:40<40:08,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7587, 'grad_norm': 1.4885323101643704, 'learning_rate': 4.800429647908354e-05, 'epoch': 1.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 170/1160 [07:04<39:42,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.76, 'grad_norm': 1.5723562472196464, 'learning_rate': 4.772786393044726e-05, 'epoch': 1.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 180/1160 [07:29<40:20,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8431, 'grad_norm': 1.6538413934214073, 'learning_rate': 4.743441185479297e-05, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 190/1160 [07:54<40:48,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7738, 'grad_norm': 1.6874761677274297, 'learning_rate': 4.712416000068771e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7658, 'grad_norm': 1.6281152176041953, 'learning_rate': 4.6797340697039705e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3984] 2025-04-11 17:26:46,442 >> Saving model checkpoint to /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-200\n",
      "[INFO|configuration_utils.py:691] 2025-04-11 17:26:46,462 >> loading configuration file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-11 17:26:46,463 >> Model config Qwen2Config {\n",
      "ures\": [tect\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "ken_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "mediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "\"qwen2\",_type\": \n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "ie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.1\",\n",
      "ache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-04-11 17:26:46,470 >> tokenizer config file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-04-11 17:26:46,470 >> Special tokens file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-200/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-11 17:26:46,764] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!\n",
      "[2025-04-11 17:26:46,774] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
      ":save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt...\n",
      "[2025-04-11 17:26:46,782] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt.\n",
      "[2025-04-11 17:26:46,783] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-04-11 17:26:46,789] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-04-11 17:26:46,789] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-04-11 17:26:46,802] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 210/1160 [08:44<38:22,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7631, 'grad_norm': 1.6197329571010664, 'learning_rate': 4.645419867912128e-05, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7528, 'grad_norm': 1.697071980532047, 'learning_rate': 4.609499090530136e-05, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 230/1160 [09:32<38:15,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7703, 'grad_norm': 2.0014198043364626, 'learning_rate': 4.5719986364624866e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 240/1160 [09:57<37:27,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0252, 'grad_norm': 1.6063810478011862, 'learning_rate': 4.532946587538302e-05, 'epoch': 2.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 250/1160 [10:22<37:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7908, 'grad_norm': 1.6444626170063508, 'learning_rate': 4.492372187482545e-05, 'epoch': 2.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 260/1160 [10:46<36:30,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7298, 'grad_norm': 1.9102957138736603, 'learning_rate': 4.450305820017156e-05, 'epoch': 2.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 270/1160 [11:10<36:05,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7752, 'grad_norm': 1.8905604336991448, 'learning_rate': 4.4067789861085185e-05, 'epoch': 2.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7316, 'grad_norm': 1.9895545425386612, 'learning_rate': 4.3618242803782825e-05, 'epoch': 2.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6923, 'grad_norm': 1.5793276175530315, 'learning_rate': 4.315475366695217e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 300/1160 [12:23<34:33,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6984, 'grad_norm': 1.7858995347607518, 'learning_rate': 4.267766952966369e-05, 'epoch': 2.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 310/1160 [12:47<34:02,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7056, 'grad_norm': 1.9921913642245834, 'learning_rate': 4.2187347651464055e-05, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7464, 'grad_norm': 1.866135304069768, 'learning_rate': 4.1684155204845974e-05, 'epoch': 2.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 330/1160 [13:36<33:50,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.727, 'grad_norm': 2.0075566665672913, 'learning_rate': 4.1168469000294895e-05, 'epoch': 2.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6736, 'grad_norm': 1.9835722583166966, 'learning_rate': 4.064067520411831e-05, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1298, 'grad_norm': 2.0002083909783583, 'learning_rate': 4.010116904926907e-05, 'epoch': 3.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 360/1160 [14:49<32:18,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7298, 'grad_norm': 1.8379424272033729, 'learning_rate': 3.955035453937935e-05, 'epoch': 3.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 370/1160 [15:13<31:42,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5663, 'grad_norm': 2.022827147543157, 'learning_rate': 3.8988644146226606e-05, 'epoch': 3.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 380/1160 [15:37<32:09,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7438, 'grad_norm': 2.0019091937794307, 'learning_rate': 3.841645850085831e-05, 'epoch': 3.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 390/1160 [16:01<31:24,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6905, 'grad_norm': 2.1544596764169883, 'learning_rate': 3.783422607860681e-05, 'epoch': 3.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 400/1160 [16:26<30:26,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7303, 'grad_norm': 2.31664738620814, 'learning_rate': 3.724238287822991e-05, 'epoch': 3.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3984] 2025-04-11 17:34:53,836 >> Saving model checkpoint to /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-400\n",
      "[INFO|configuration_utils.py:691] 2025-04-11 17:34:53,852 >> loading configuration file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-11 17:34:53,853 >> Model config Qwen2Config {\n",
      "ures\": [tect\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "ken_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "mediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "\"qwen2\",_type\": \n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "ie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.1\",\n",
      "ache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-04-11 17:34:53,860 >> tokenizer config file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-04-11 17:34:53,860 >> Special tokens file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-400/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-11 17:34:54,115] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!\n",
      "[2025-04-11 17:34:54,126] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
      ":save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_model_states.pt...\n",
      "[2025-04-11 17:34:54,132] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_model_states.pt.\n",
      "[2025-04-11 17:34:54,133] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-04-11 17:34:54,139] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-04-11 17:34:54,139] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-04-11 17:34:54,152] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 410/1160 [16:52<30:34,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7066, 'grad_norm': 1.9407891944266895, 'learning_rate': 3.66413720954177e-05, 'epoch': 3.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 420/1160 [17:16<29:52,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7082, 'grad_norm': 1.8822784724434696, 'learning_rate': 3.603164379091006e-05, 'epoch': 3.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 430/1160 [17:40<30:18,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6322, 'grad_norm': 2.1538549589583664, 'learning_rate': 3.541365455347327e-05, 'epoch': 3.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 440/1160 [18:05<29:22,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6764, 'grad_norm': 1.7874620839553097, 'learning_rate': 3.478786715798823e-05, 'epoch': 3.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 450/1160 [18:29<29:01,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5873, 'grad_norm': 1.9044539102322484, 'learning_rate': 3.415475021890622e-05, 'epoch': 3.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 460/1160 [18:54<28:11,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6342, 'grad_norm': 2.080121765250745, 'learning_rate': 3.3514777839331856e-05, 'epoch': 3.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 470/1160 [19:18<27:53,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0377, 'grad_norm': 2.1257549403934006, 'learning_rate': 3.286842925599579e-05, 'epoch': 4.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.64, 'grad_norm': 2.068834472743256, 'learning_rate': 3.2216188480383256e-05, 'epoch': 4.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6829, 'grad_norm': 1.7406246691060183, 'learning_rate': 3.1558543936287035e-05, 'epoch': 4.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 500/1160 [20:32<27:00,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.597, 'grad_norm': 1.7947476051613653, 'learning_rate': 3.089598809405633e-05, 'epoch': 4.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 510/1160 [20:56<26:37,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7086, 'grad_norm': 1.9463890759502533, 'learning_rate': 3.022901710181542e-05, 'epoch': 4.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5848, 'grad_norm': 2.2435159087829075, 'learning_rate': 2.955813041392822e-05, 'epoch': 4.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 530/1160 [21:44<25:35,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6768, 'grad_norm': 1.847937501802155, 'learning_rate': 2.888383041698704e-05, 'epoch': 4.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 540/1160 [22:09<24:59,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6512, 'grad_norm': 2.338418103397101, 'learning_rate': 2.8206622053605553e-05, 'epoch': 4.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 550/1160 [22:33<24:26,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6472, 'grad_norm': 1.9051405809147652, 'learning_rate': 2.7527012444297707e-05, 'epoch': 4.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 560/1160 [22:57<24:20,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6316, 'grad_norm': 2.451749843762721, 'learning_rate': 2.6845510507725745e-05, 'epoch': 4.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 570/1160 [23:21<23:39,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6624, 'grad_norm': 2.244367227289835, 'learning_rate': 2.616262657960173e-05, 'epoch': 4.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 580/1160 [23:46<23:26,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6996, 'grad_norm': 2.1924579503126096, 'learning_rate': 2.5478872030527855e-05, 'epoch': 4.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 590/1160 [24:10<22:51,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0211, 'grad_norm': 1.7174074656386609, 'learning_rate': 2.479475888306186e-05, 'epoch': 5.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 600/1160 [24:34<22:37,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6878, 'grad_norm': 2.5712622605522837, 'learning_rate': 2.411079942829421e-05, 'epoch': 5.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3984] 2025-04-11 17:43:02,323 >> Saving model checkpoint to /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-600\n",
      "[INFO|configuration_utils.py:691] 2025-04-11 17:43:02,339 >> loading configuration file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-11 17:43:02,340 >> Model config Qwen2Config {\n",
      "ures\": [tect\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "ken_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "mediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "\"qwen2\",_type\": \n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "ie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.1\",\n",
      "ache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-04-11 17:43:02,347 >> tokenizer config file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-04-11 17:43:02,347 >> Special tokens file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-600/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-11 17:43:02,604] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step601 is about to be saved!\n",
      "[2025-04-11 17:43:02,614] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-600/global_step601/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
      "[2025-04-11 17:43:02,614] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-600/global_step601/zero_pp_rank_0_mp_rank_00_model_states.pt...\n",
      "[2025-04-11 17:43:02,622] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-600/global_step601/zero_pp_rank_0_mp_rank_00_model_states.pt.\n",
      "[2025-04-11 17:43:02,625] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-600/global_step601/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-04-11 17:43:02,631] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-600/global_step601/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-04-11 17:43:02,631] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-600/global_step601/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-04-11 17:43:02,644] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step601 is ready now!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6357, 'grad_norm': 1.9536251765059294, 'learning_rate': 2.3427505842224154e-05, 'epoch': 5.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 620/1160 [25:24<21:38,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6891, 'grad_norm': 2.66333130827071, 'learning_rate': 2.2745389802222032e-05, 'epoch': 5.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5237, 'grad_norm': 2.0149097037404555, 'learning_rate': 2.2064962103864937e-05, 'epoch': 5.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 640/1160 [26:13<20:44,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6166, 'grad_norm': 2.384319766486753, 'learning_rate': 2.1386732278432676e-05, 'epoch': 5.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5623, 'grad_norm': 1.923909857259892, 'learning_rate': 2.071120821135054e-05, 'epoch': 5.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 660/1160 [27:01<19:57,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7157, 'grad_norm': 2.28414724546421, 'learning_rate': 2.003889576186455e-05, 'epoch': 5.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 670/1160 [27:25<20:09,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6362, 'grad_norm': 2.1755484615541634, 'learning_rate': 1.937029838423389e-05, 'epoch': 5.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 680/1160 [27:50<19:28,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6583, 'grad_norm': 1.8905943237967873, 'learning_rate': 1.870591675072446e-05, 'epoch': 5.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5556, 'grad_norm': 2.3191372166906716, 'learning_rate': 1.804624837668553e-05, 'epoch': 5.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 700/1160 [28:39<19:07,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9513, 'grad_norm': 2.4993110638224385, 'learning_rate': 1.7391787247990538e-05, 'epoch': 6.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 710/1160 [29:03<18:12,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6, 'grad_norm': 2.3144174432343556, 'learning_rate': 1.6743023451120832e-05, 'epoch': 6.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6396, 'grad_norm': 2.0634955285011327, 'learning_rate': 1.6100442806169422e-05, 'epoch': 6.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6809, 'grad_norm': 2.4708210478380828, 'learning_rate': 1.5464526503039666e-05, 'epoch': 6.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 740/1160 [30:17<17:21,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6282, 'grad_norm': 2.305327604000299, 'learning_rate': 1.4835750741111138e-05, 'epoch': 6.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 750/1160 [30:42<16:35,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5402, 'grad_norm': 2.1428268885942234, 'learning_rate': 1.4214586372642563e-05, 'epoch': 6.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 760/1160 [31:06<16:16,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6467, 'grad_norm': 2.0593936957539616, 'learning_rate': 1.360149855017906e-05, 'epoch': 6.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 770/1160 [31:30<15:43,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5924, 'grad_norm': 2.1040635627548894, 'learning_rate': 1.2996946378227352e-05, 'epoch': 6.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6354, 'grad_norm': 2.0116172354201867, 'learning_rate': 1.2401382569460119e-05, 'epoch': 6.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 790/1160 [32:19<14:53,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6138, 'grad_norm': 2.544326623809307, 'learning_rate': 1.181525310570677e-05, 'epoch': 6.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 800/1160 [32:43<14:55,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6379, 'grad_norm': 2.3441031986141034, 'learning_rate': 1.1238996903984537e-05, 'epoch': 6.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3984] 2025-04-11 17:51:11,579 >> Saving model checkpoint to /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-800\n",
      "[INFO|configuration_utils.py:691] 2025-04-11 17:51:11,596 >> loading configuration file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-11 17:51:11,597 >> Model config Qwen2Config {\n",
      "ures\": [tect\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "ken_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "mediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "\"qwen2\",_type\": \n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "ie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.1\",\n",
      "ache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-04-11 17:51:11,604 >> tokenizer config file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-04-11 17:51:11,604 >> Special tokens file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-800/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-11 17:51:11,883] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step801 is about to be saved!\n",
      "[2025-04-11 17:51:11,894] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-800/global_step801/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
      "[2025-04-11 17:51:11,894] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-800/global_step801/zero_pp_rank_0_mp_rank_00_model_states.pt...\n",
      "[2025-04-11 17:51:11,901] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-800/global_step801/zero_pp_rank_0_mp_rank_00_model_states.pt.\n",
      "[2025-04-11 17:51:11,901] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-800/global_step801/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-04-11 17:51:11,908] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-800/global_step801/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-04-11 17:51:11,908] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-800/global_step801/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-04-11 17:51:11,921] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step801 is ready now!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 810/1160 [33:10<14:23,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5769, 'grad_norm': 2.2478082470338396, 'learning_rate': 1.0673045487819975e-05, 'epoch': 6.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 820/1160 [33:34<13:43,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9903, 'grad_norm': 2.0589048827902454, 'learning_rate': 1.0117822664107038e-05, 'epoch': 7.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.653, 'grad_norm': 2.1644924563106938, 'learning_rate': 9.573744205743612e-06, 'epoch': 7.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6927, 'grad_norm': 2.248342774948777, 'learning_rate': 9.041217540284277e-06, 'epoch': 7.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 850/1160 [34:47<12:32,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.653, 'grad_norm': 1.9744541151121933, 'learning_rate': 8.520641444842373e-06, 'epoch': 7.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 860/1160 [35:12<12:14,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6588, 'grad_norm': 2.5996444419100744, 'learning_rate': 8.012405747469862e-06, 'epoch': 7.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6021, 'grad_norm': 2.22488351581885, 'learning_rate': 7.516891035238596e-06, 'epoch': 7.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 880/1160 [36:00<11:02,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5226, 'grad_norm': 1.855913725892829, 'learning_rate': 7.034468369241651e-06, 'epoch': 7.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 890/1160 [36:24<10:35,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6153, 'grad_norm': 1.9925081258693704, 'learning_rate': 6.565499006727938e-06, 'epoch': 7.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 900/1160 [36:47<10:07,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6095, 'grad_norm': 2.263869686632483, 'learning_rate': 6.1103341305785655e-06, 'epoch': 7.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 910/1160 [37:11<09:48,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5794, 'grad_norm': 2.1713206568086876, 'learning_rate': 5.669314586327054e-06, 'epoch': 7.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 920/1160 [37:34<09:24,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5484, 'grad_norm': 2.202840528281682, 'learning_rate': 5.242770626920695e-06, 'epoch': 7.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 930/1160 [37:58<09:30,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0168, 'grad_norm': 1.9459930694267211, 'learning_rate': 4.8310216654140425e-06, 'epoch': 8.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 940/1160 [38:21<08:20,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5134, 'grad_norm': 2.155133843347423, 'learning_rate': 4.4343760357797386e-06, 'epoch': 8.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 950/1160 [38:44<07:52,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5152, 'grad_norm': 2.1759282410258334, 'learning_rate': 4.053130762015736e-06, 'epoch': 8.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 960/1160 [39:07<07:51,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6336, 'grad_norm': 2.135701837358262, 'learning_rate': 3.687571335722023e-06, 'epoch': 8.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5355, 'grad_norm': 2.0613958260087015, 'learning_rate': 3.337971502313095e-06, 'epoch': 8.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 980/1160 [39:54<06:46,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6398, 'grad_norm': 1.9175221368721043, 'learning_rate': 3.0045930560265666e-06, 'epoch': 8.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 990/1160 [40:16<06:24,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6498, 'grad_norm': 2.097744776547409, 'learning_rate': 2.6876856438812296e-06, 'epoch': 8.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 1000/1160 [40:39<06:01,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7583, 'grad_norm': 2.370966665503608, 'learning_rate': 2.3874865787314598e-06, 'epoch': 8.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3984] 2025-04-11 17:59:06,895 >> Saving model checkpoint to /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-1000\n",
      "[INFO|configuration_utils.py:691] 2025-04-11 17:59:06,911 >> loading configuration file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-11 17:59:06,911 >> Model config Qwen2Config {\n",
      "ures\": [tect\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "ken_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "mediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "\"qwen2\",_type\": \n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "ie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.1\",\n",
      "ache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-04-11 17:59:06,918 >> tokenizer config file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-04-11 17:59:06,918 >> Special tokens file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-1000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-11 17:59:07,122] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1002 is about to be saved!\n",
      "[2025-04-11 17:59:07,132] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-1000/global_step1002/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
      "[2025-04-11 17:59:07,132] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-1000/global_step1002/zero_pp_rank_0_mp_rank_00_model_states.pt...\n",
      "[2025-04-11 17:59:07,138] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-1000/global_step1002/zero_pp_rank_0_mp_rank_00_model_states.pt.\n",
      "[2025-04-11 17:59:07,139] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-1000/global_step1002/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-04-11 17:59:07,145] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-1000/global_step1002/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-04-11 17:59:07,145] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-1000/global_step1002/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-04-11 17:59:07,156] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1002 is ready now!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 1010/1160 [41:03<05:39,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5996, 'grad_norm': 2.6797604689072503, 'learning_rate': 2.1042206615579237e-06, 'epoch': 8.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 1020/1160 [41:25<05:15,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5907, 'grad_norm': 1.877813490211035, 'learning_rate': 1.8381000131277e-06, 'epoch': 8.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6071, 'grad_norm': 1.8178394003756657, 'learning_rate': 1.5893239151497652e-06, 'epoch': 8.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 1040/1160 [42:11<04:29,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5257, 'grad_norm': 2.402601745045876, 'learning_rate': 1.3580786610450202e-06, 'epoch': 8.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 1050/1160 [42:34<04:12,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0565, 'grad_norm': 2.1676583840856614, 'learning_rate': 1.144537416442315e-06, 'epoch': 9.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 1060/1160 [42:57<03:47,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5751, 'grad_norm': 2.207080684987098, 'learning_rate': 9.488600895051714e-07, 'epoch': 9.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1070/1160 [43:19<03:22,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5852, 'grad_norm': 2.093726664844737, 'learning_rate': 7.711932111862025e-07, 'epoch': 9.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 1080/1160 [43:42<02:59,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6758, 'grad_norm': 2.120137553952987, 'learning_rate': 6.116698254989256e-07, 'epoch': 9.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 1090/1160 [44:04<02:36,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6258, 'grad_norm': 2.111068977102907, 'learning_rate': 4.704093898890871e-07, 'epoch': 9.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 1100/1160 [44:26<02:14,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5568, 'grad_norm': 2.2472842220822185, 'learning_rate': 3.475176857802298e-07, 'epoch': 9.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 1110/1160 [44:49<01:52,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.629, 'grad_norm': 2.286923568249665, 'learning_rate': 2.4308673936032646e-07, 'epoch': 9.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 1120/1160 [45:12<01:30,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5186, 'grad_norm': 1.9510569305748136, 'learning_rate': 1.571947526689349e-07, 'epoch': 9.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 1130/1160 [45:34<01:07,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6675, 'grad_norm': 2.020701831942913, 'learning_rate': 8.990604503639477e-08, 'epoch': 9.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 1140/1160 [45:57<00:45,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5826, 'grad_norm': 2.108085865149144, 'learning_rate': 4.1271004918971847e-08, 'epoch': 9.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 1150/1160 [46:20<00:22,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5751, 'grad_norm': 2.405836790612898, 'learning_rate': 1.1326052165960831e-08, 'epoch': 9.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1160/1160 [46:43<00:00,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6137, 'grad_norm': 1.9717266968949296, 'learning_rate': 9.361074708169604e-11, 'epoch': 9.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3984] 2025-04-11 18:05:10,817 >> Saving model checkpoint to /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-1160\n",
      "[INFO|configuration_utils.py:691] 2025-04-11 18:05:10,833 >> loading configuration file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-11 18:05:10,833 >> Model config Qwen2Config {\n",
      "ures\": [tect\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "ken_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "mediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "\"qwen2\",_type\": \n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "ie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.1\",\n",
      "ache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-04-11 18:05:10,840 >> tokenizer config file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-1160/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-04-11 18:05:10,840 >> Special tokens file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-1160/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-11 18:05:11,043] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1162 is about to be saved!\n",
      "[2025-04-11 18:05:11,052] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-1160/global_step1162/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
      "[2025-04-11 18:05:11,052] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-1160/global_step1162/zero_pp_rank_0_mp_rank_00_model_states.pt...\n",
      "[2025-04-11 18:05:11,059] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-1160/global_step1162/zero_pp_rank_0_mp_rank_00_model_states.pt.\n",
      "[2025-04-11 18:05:11,059] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-1160/global_step1162/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-04-11 18:05:11,066] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-1160/global_step1162/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-04-11 18:05:11,066] [INFO] [engine.py:3672:_save_zero_checkpoint] zero checkpoint saved /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/checkpoint-1160/global_step1162/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-04-11 18:05:11,077] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1162 is ready now!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2681] 2025-04-11 18:05:11,081 >> \n",
      "\n",
      "model on huggingface.co/models =) to share your \n",
      "\n",
      "\n",
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2804.6659, 'train_samples_per_second': 13.296, 'train_steps_per_second': 0.414, 'train_loss': 3.7280727550901216, 'epoch': 9.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1160/1160 [46:44<00:00,  2.42s/it]\n",
      "[INFO|trainer.py:3984] 2025-04-11 18:05:12,693 >> Saving model checkpoint to /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan\n",
      "[INFO|configuration_utils.py:691] 2025-04-11 18:05:12,707 >> loading configuration file /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-11 18:05:12,708 >> Model config Qwen2Config {\n",
      "ures\": [tect\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "ken_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "mediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "\"qwen2\",_type\": \n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "ie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.1\",\n",
      "ache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-04-11 18:05:12,714 >> tokenizer config file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-04-11 18:05:12,714 >> Special tokens file saved in /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =     9.9936\n",
      "  total_flos               =    13688GF\n",
      "  train_loss               =     3.7281\n",
      "        = 0:46:44.66\n",
      "  train_samples_per_second =     13.296\n",
      "  train_steps_per_second   =      0.414\n",
      "Figure saved at: /home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan/training_loss.png\n",
      "[WARNING|2025-04-11 18:05:13] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
      "NG|2025-04-11 18:05:13] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modelcard.py:450] 2025-04-11 18:05:13,040 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "#!/bin/bash\n",
    "export CUDA_VISIBLE_DEVICES=3,7\n",
    "\n",
    "# 设置节点数为1\n",
    "export NNODES=1\n",
    "# 设置每个节点上的GPU数量为1\n",
    "export GPUS_PER_NODE=2\n",
    "# 设置当前节点的等级为0\n",
    "export NODE_RANK=0\n",
    "# 设置主节点的地址为本地IP\n",
    "export MASTER_ADDR=localhost\n",
    "# 设置主节点的端口为1234\n",
    "export MASTER_PORT=1234\n",
    "\n",
    "MODEL='/home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct/'\n",
    "OUTPUT_PATH='/home/zhangsh82/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-zhenhuan'\n",
    "DATA_SET='zhenhuan'\n",
    "DS_CONFIG_PATH='/home/zhangsh82/data/zsh/LLaMA-Factory/examples/deepspeed/ds_z3_config.json'\n",
    "\n",
    "\n",
    "DISTRIBUTED_ARGS=\"\n",
    "    --nproc_per_node $GPUS_PER_NODE \\\n",
    "    --nnodes $NNODES \\\n",
    "    --node_rank $NODE_RANK \\\n",
    "    --master_addr $MASTER_ADDR \\\n",
    "    --master_port $MASTER_PORT\n",
    "\"\n",
    "\n",
    "torchrun $DISTRIBUTED_ARGS src/train.py \\\n",
    "    --deepspeed $DS_CONFIG_PATH \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --use_fast_tokenizer \\\n",
    "    --flash_attn disabled \\\n",
    "    --model_name_or_path $MODEL \\\n",
    "    --dataset $DATA_SET \\\n",
    "    --template qwen \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target q_proj,v_proj\\\n",
    "    --output_dir $OUTPUT_PATH \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --warmup_ratio 0.01 \\\n",
    "    --weight_decay 0.1 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --ddp_timeout 9000 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 10 \\\n",
    "    --cutoff_len 4096 \\\n",
    "    --save_steps 200 \\\n",
    "    --plot_loss True\\\n",
    "    --num_train_epochs 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df1815-9e1c-453f-87b5-5333a4f89dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
